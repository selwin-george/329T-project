{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_perturbations_to_label.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkpLh5HX2Rkh",
        "outputId": "82053aca-dc6d-46c1-e293-5bca3193ba26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/Shareddrives/329T/Project/"
      ],
      "metadata": {
        "id": "ZamgC_7c8asE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd16e6a-9c5c-41a9-add0-7818feb0b008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/Shareddrives/329T/Project/'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/329T Project/"
      ],
      "metadata": {
        "id": "gauD3Dazsxjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf3c61f-742d-42d6-8427-b83e2a0ed7a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/329T Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages \n",
        "!pip3 install transformers \n",
        "!pip3 install torch"
      ],
      "metadata": {
        "id": "01GaEkoQ1BiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8857fd41-1687-4847-c2c0-017e0aa48317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from transformers import BertConfig, BertTokenizer, AutoConfig, AutoTokenizer, DistilBertConfig, DistilBertTokenizer\n",
        "from transformers import BertForSequenceClassification, BertForMaskedLM, AutoModelForSequenceClassification, DistilBertForMaskedLM\n",
        "import copy\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import copy"
      ],
      "metadata": {
        "id": "LGIKzlS78h5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import importlib\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "def install_if_not_installed(packages):\n",
        "  \"\"\"Install the given packages if they are not already installed.\"\"\"\n",
        "\n",
        "  for package in packages:\n",
        "    if isinstance(package, tuple):\n",
        "      package_name, package_package = package\n",
        "    else:\n",
        "      package_name = package\n",
        "      package_package = package\n",
        "\n",
        "    print(f\"{package_name} ... \", end='')\n",
        "\n",
        "    try:\n",
        "      importlib.import_module(package_name)\n",
        "      print(\"already installed\")\n",
        "\n",
        "    except:\n",
        "      print(f\"installing from {package_package}\")\n",
        "      subprocess.check_call(\n",
        "          [sys.executable, \"-m\", \"pip\", \"install\", package_package]\n",
        "      )\n",
        "\n",
        "install_if_not_installed(\n",
        "    [(\"trulens\", \"git+https://github.com/truera/trulens.git@piotrm/vis/output-detect\"),\n",
        "     \"pandas\",\n",
        "     \"numpy\",\n",
        "     \"domonic\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLiD5-poXfCD",
        "outputId": "fed91541-c0b3-4120-8c26-edff82298ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trulens ... already installed\n",
            "pandas ... already installed\n",
            "numpy ... already installed\n",
            "domonic ... already installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trulens.nn.attribution import IntegratedGradients\n",
        "from trulens.nn.models import get_model_wrapper\n",
        "from trulens.nn.attribution import Cut, OutputCut\n",
        "from trulens.nn.quantities import MaxClassQoI, ClassQoI, ComparativeQoI\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from trulens.visualizations import NLP"
      ],
      "metadata": {
        "id": "gd1D8XVxXhVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Args"
      ],
      "metadata": {
        "id": "eZRnaWPQvhIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "data_path = \"tweet_sentiment_pairs.csv\"\n",
        "mlm_path = \"distilbert-base-uncased\"\n",
        "tgt_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "output_dir = \"perturbations.csv\"\n",
        "use_sim_mat = 0 \n",
        "start = 0\n",
        "end = 100\n",
        "num_label = 2 \n",
        "use_bpe = 1\n",
        "k = 5\n",
        "threshold_pred_score = 0"
      ],
      "metadata": {
        "id": "VHPlomgMX10w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Initialize models"
      ],
      "metadata": {
        "id": "WhZT2vdKvjWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_mlm = DistilBertTokenizer.from_pretrained(mlm_path, do_lower_case=True)\n",
        "tokenizer_tgt = AutoTokenizer.from_pretrained(tgt_path, do_lower_case=True)\n",
        "\n",
        "config_atk = DistilBertConfig.from_pretrained(mlm_path)\n",
        "mlm_model = DistilBertForMaskedLM.from_pretrained(mlm_path, config=config_atk)\n",
        "mlm_model.to(device)\n",
        "\n",
        "config_tgt = AutoConfig.from_pretrained(tgt_path, num_labels=num_label)\n",
        "tgt_model = AutoModelForSequenceClassification.from_pretrained(tgt_path, config=config_tgt)\n",
        "tgt_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGoSlqSPXpyQ",
        "outputId": "5f6d1693-e059-41c3-e8c2-2976454dd24d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For pytorch, input_shape is required... I just used the tokenizer max_length here\n",
        "wrapped_model = get_model_wrapper(tgt_model, input_shape=(None, 128), device=device)"
      ],
      "metadata": {
        "id": "kSKYjWupXpP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43bd652-8267-45ae-df09-8e78e375f674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: get_model_wrapper: input_shape parameter is no longer used and will be removed in the future\n",
            "INFO: Detected pytorch backend for <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'>.\n",
            "INFO: Using backend Backend.PYTORCH.\n",
            "INFO: If this seems incorrect, you can force the correct backend by passing the `backend` parameter directly into your get_model_wrapper call.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UCuvhhhBYDSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attack code from BERT-Attack"
      ],
      "metadata": {
        "id": "xM5akvVbOu5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "filter_words = ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'ain', 'all', 'almost',\n",
        "                'alone', 'along', 'already', 'also', 'although', 'am', 'among', 'amongst', 'an', 'and', 'another',\n",
        "                'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'aren', \"aren't\", 'around', 'as',\n",
        "                'at', 'back', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides',\n",
        "                'between', 'beyond', 'both', 'but', 'by', 'can', 'cannot', 'could', 'couldn', \"couldn't\", 'd', 'didn',\n",
        "                \"didn't\", 'doesn', \"doesn't\", 'don', \"don't\", 'down', 'due', 'during', 'either', 'else', 'elsewhere',\n",
        "                'empty', 'enough', 'even', 'ever', 'everyone', 'everything', 'everywhere', 'except', 'first', 'for',\n",
        "                'former', 'formerly', 'from', 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'he', 'hence',\n",
        "                'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his',\n",
        "                'how', 'however', 'hundred', 'i', 'if', 'in', 'indeed', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\",\n",
        "                'its', 'itself', 'just', 'latter', 'latterly', 'least', 'll', 'may', 'me', 'meanwhile', 'mightn',\n",
        "                \"mightn't\", 'mine', 'more', 'moreover', 'most', 'mostly', 'must', 'mustn', \"mustn't\", 'my', 'myself',\n",
        "                'namely', 'needn', \"needn't\", 'neither', 'never', 'nevertheless', 'next', 'no', 'nobody', 'none',\n",
        "                'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'o', 'of', 'off', 'on', 'once', 'one', 'only',\n",
        "                'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'per',\n",
        "                'please', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", \"should've\", 'shouldn', \"shouldn't\", 'somehow',\n",
        "                'something', 'sometime', 'somewhere', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs',\n",
        "                'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein',\n",
        "                'thereupon', 'these', 'they', 'this', 'those', 'through', 'throughout', 'thru', 'thus', 'to', 'too',\n",
        "                'toward', 'towards', 'under', 'unless', 'until', 'up', 'upon', 'used', 've', 'was', 'wasn', \"wasn't\",\n",
        "                'we', 'were', 'weren', \"weren't\", 'what', 'whatever', 'when', 'whence', 'whenever', 'where',\n",
        "                'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while',\n",
        "                'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'with', 'within', 'without', 'won',\n",
        "                \"won't\", 'would', 'wouldn', \"wouldn't\", 'y', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
        "                'your', 'yours', 'yourself', 'yourselves', '-', ',', '.', ':', ';', '(', ')', '\"', \"'\", '|', '...']\n",
        "filter_words = set(filter_words)\n",
        "\n",
        "\n",
        "def get_sim_embed(embed_path, sim_path):\n",
        "    id2word = {}\n",
        "    word2id = {}\n",
        "\n",
        "    with open(embed_path, 'r', encoding='utf-8') as ifile:\n",
        "        for line in ifile:\n",
        "            word = line.split()[0]\n",
        "            if word not in id2word:\n",
        "                id2word[len(id2word)] = word\n",
        "                word2id[word] = len(id2word) - 1\n",
        "\n",
        "    cos_sim = np.load(sim_path)\n",
        "    return cos_sim, word2id, id2word\n",
        "\n",
        "\n",
        "\n",
        "class Feature(object):\n",
        "    def __init__(self, seq_a, label, example_index):\n",
        "        self.example_index = example_index\n",
        "        self.label = label\n",
        "        self.pred = -1\n",
        "        self.seq = seq_a\n",
        "        self.final_adverse = seq_a\n",
        "        self.query = 0\n",
        "        self.change = 0\n",
        "        self.success = 0\n",
        "        self.sim = 0.0\n",
        "        self.metrics = None\n",
        "        self.changes = []\n",
        "        self.attributions = None\n",
        "\n",
        "\n",
        "def _tokenize(seq, tokenizer):\n",
        "   seq = seq.replace('\\n', '').lower()\n",
        "   words = seq.split(' ')\n",
        " \n",
        "   sub_words = []\n",
        "   keys = []\n",
        "   index = 0\n",
        "   for word in words:\n",
        "       sub = tokenizer.tokenize(word)\n",
        "       sub_words += sub\n",
        "       keys.append([index, index + len(sub)])\n",
        "       index += len(sub)\n",
        " \n",
        "   return words, sub_words, keys\n",
        " \n",
        " \n",
        "def _get_masked(words):\n",
        "   len_text = len(words)\n",
        "   masked_words = []\n",
        "   for i in range(len_text - 1):\n",
        "       masked_words.append(words[0:i] + ['[UNK]'] + words[i + 1:])\n",
        "   # list of words\n",
        "   return masked_words\n",
        " \n",
        " \n",
        "def get_important_scores(words, tgt_model, orig_prob, orig_label, orig_probs, tokenizer_tgt, batch_size, max_length):\n",
        "   masked_words = _get_masked(words)\n",
        "   texts = [' '.join(words) for words in masked_words]  # list of text of masked words\n",
        "   all_input_ids = []\n",
        "   all_masks = []\n",
        "   all_segs = []\n",
        "   for text in texts:\n",
        "       inputs = tokenizer_tgt.encode_plus(text, None, add_special_tokens=True, max_length=max_length, return_token_type_ids=True, truncation=True)\n",
        "       input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
        "       attention_mask = [1] * len(input_ids)\n",
        "       padding_length = max_length - len(input_ids)\n",
        "       input_ids = input_ids + (padding_length * [0])\n",
        "       token_type_ids = token_type_ids + (padding_length * [0])\n",
        "       attention_mask = attention_mask + (padding_length * [0])\n",
        "       all_input_ids.append(input_ids)\n",
        "       all_masks.append(attention_mask)\n",
        "       all_segs.append(token_type_ids)\n",
        "   seqs = torch.tensor(all_input_ids, dtype=torch.long)\n",
        "   masks = torch.tensor(all_masks, dtype=torch.long)\n",
        "   segs = torch.tensor(all_segs, dtype=torch.long)\n",
        "   seqs = seqs.to(device)\n",
        " \n",
        "   eval_data = TensorDataset(seqs)\n",
        "   # Run prediction for full data\n",
        "   eval_sampler = SequentialSampler(eval_data)\n",
        "   eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=batch_size)\n",
        "   leave_1_probs = []\n",
        "   for batch in eval_dataloader:\n",
        "       masked_input, = batch\n",
        "       bs = masked_input.size(0)\n",
        " \n",
        "       leave_1_prob_batch = tgt_model(masked_input)[0]  # B num-label\n",
        "       leave_1_probs.append(leave_1_prob_batch)\n",
        "   leave_1_probs = torch.cat(leave_1_probs, dim=0)  # words, num-label\n",
        "   leave_1_probs = torch.softmax(leave_1_probs, -1)  #\n",
        "   leave_1_probs_argmax = torch.argmax(leave_1_probs, dim=-1)\n",
        "   import_scores = (orig_prob\n",
        "                    - leave_1_probs[:, orig_label]\n",
        "                    +\n",
        "                    (leave_1_probs_argmax != orig_label).float()\n",
        "                    * (leave_1_probs.max(dim=-1)[0] - torch.index_select(orig_probs, 0, leave_1_probs_argmax))\n",
        "                    ).data.cpu().numpy()\n",
        " \n",
        "   return import_scores\n",
        " \n",
        " \n",
        "def get_substitues(substitutes, tokenizer, mlm_model, use_bpe, substitutes_score=None, threshold=3.0):\n",
        "   # substitues L,k\n",
        "   # from this matrix to recover a word\n",
        "   words = []\n",
        "   sub_len, k = substitutes.size()  # sub-len, k\n",
        " \n",
        "   if sub_len == 0:\n",
        "       return words\n",
        " \n",
        "   elif sub_len == 1:\n",
        "       for (i,j) in zip(substitutes[0], substitutes_score[0]):\n",
        "           if threshold != 0 and j < threshold:\n",
        "               break\n",
        "           words.append(tokenizer._convert_id_to_token(int(i)))\n",
        "   return words\n",
        " \n",
        " \n",
        "def get_bpe_substitues(substitutes, tokenizer, mlm_model):\n",
        "   # substitutes L, k\n",
        " \n",
        "   substitutes = substitutes[0:5, 0:4] # maximum BPE candidates # 0:12\n",
        " \n",
        "   # find all possible candidates\n",
        " \n",
        "   all_substitutes = []\n",
        "   for i in range(substitutes.size(0)):\n",
        "       if len(all_substitutes) == 0:\n",
        "           lev_i = substitutes[i]\n",
        "           all_substitutes = [[int(c)] for c in lev_i]\n",
        "       else:\n",
        "           lev_i = []\n",
        "           for all_sub in all_substitutes:\n",
        "               for j in substitutes[i]:\n",
        "                   lev_i.append(all_sub + [int(j)])\n",
        "           all_substitutes = lev_i\n",
        " \n",
        "   # all substitutes  list of list of token-id (all candidates)\n",
        "   c_loss = nn.CrossEntropyLoss(reduction='none')\n",
        "   word_list = []\n",
        "   # all_substitutes = all_substitutes[:24]\n",
        "   all_substitutes = torch.tensor(all_substitutes) # [ N, L ]\n",
        "   all_substitutes = all_substitutes[:24].to(device)\n",
        "   N, L = all_substitutes.size()\n",
        "   word_predictions = mlm_model(all_substitutes)[0] # N L vocab-size\n",
        "   ppl = c_loss(word_predictions.view(N*L, -1), all_substitutes.view(-1)) # [ N*L ]\n",
        "   ppl = torch.exp(torch.mean(ppl.view(N, L), dim=-1)) # N\n",
        "   _, word_list = torch.sort(ppl)\n",
        "   word_list = [all_substitutes[i] for i in word_list]\n",
        "   final_words = []\n",
        "   for word in word_list:\n",
        "       tokens = [tokenizer._convert_id_to_token(int(i)) for i in word]\n",
        "       text = tokenizer.convert_tokens_to_string(tokens)\n",
        "       final_words.append(text)\n",
        "   return final_words\n",
        " \n",
        "def num_tokens(seq):\n",
        " inputs = tokenizer_tgt.encode_plus(seq, None, add_special_tokens=True, max_length=128, return_token_type_ids=True, truncation=True)\n",
        " return torch.tensor(inputs[\"input_ids\"]).size(0)\n",
        " \n",
        "def attack(feature, tgt_model, mlm_model, tokenizer_tgt, tokenizer_mlm, k, batch_size, max_length=128, cos_mat=None, w2i={}, i2w={}, use_bpe=1, threshold_pred_score=0.3):\n",
        "   features = []\n",
        " \n",
        "   # Get words, subwords, and the indices corresponding to that word in the tokenized sequence\n",
        "   words, sub_words, keys = _tokenize(feature.seq, tokenizer_tgt)\n",
        " \n",
        "   # Get original label (orig_label) and current probability (current_prob)\n",
        "   # Feed in original sequence (feature.seq) into target model\n",
        "   inputs = tokenizer_tgt.encode_plus(feature.seq, None, add_special_tokens=True, max_length=max_length, return_token_type_ids=True, truncation=True)\n",
        "   input_ids, token_type_ids = torch.tensor(inputs[\"input_ids\"]), torch.tensor(inputs[\"token_type_ids\"])\n",
        "   attention_mask = torch.tensor([1] * len(input_ids))\n",
        "   seq_len = input_ids.size(0)\n",
        "   num_tokens_original = seq_len\n",
        "   orig_probs = tgt_model(input_ids.unsqueeze(0).to(device),\n",
        "                          attention_mask=attention_mask.unsqueeze(0).to(device),\n",
        "                          )[0].squeeze()\n",
        "   orig_probs = torch.softmax(orig_probs, -1)\n",
        "   orig_label = torch.argmax(orig_probs)\n",
        "   current_prob = orig_probs.max()\n",
        " \n",
        "   # If the original label is already different from the label in the feature, it means the model is wrong already\n",
        "   if orig_label != feature.label:\n",
        "       feature.success = 3\n",
        "       features.append(copy.deepcopy(feature))\n",
        " \n",
        "   # Feed subwords into MLM model\n",
        "   sub_words = ['[CLS]'] + sub_words[:max_length - 2] + ['[SEP]']\n",
        "   input_ids_ = torch.tensor([tokenizer_mlm.convert_tokens_to_ids(sub_words)])\n",
        "   # For each subword in the sequence, outputs a prediction (vocab size vector)\n",
        "   word_predictions = mlm_model(input_ids_.to(device))[0].squeeze()  # seq-len(sub) vocab\n",
        "   # word_pred_scores_all is the top k values, word_predictions is their corresponding indices\n",
        "   word_pred_scores_all, word_predictions = torch.topk(word_predictions, k, -1)  # seq-len k\n",
        "   # Top k predictions (indices) for subwords (other than CLS and SEP)\n",
        "   word_predictions = word_predictions[1:len(sub_words) + 1, :]\n",
        "   # Top k predictions (probabilities) for subwords (other than CLS and SEP)\n",
        "   word_pred_scores_all = word_pred_scores_all[1:len(sub_words) + 1, :]\n",
        "   # Get importance scores for each word\n",
        "   important_scores = get_important_scores(words, tgt_model, current_prob, orig_label, orig_probs,\n",
        "                                           tokenizer_tgt, batch_size, max_length)\n",
        "   # Store number of words??\n",
        "   feature.query += int(len(words))\n",
        "   # Get indices of words in decreasing importance score order\n",
        "   list_of_index = sorted(enumerate(important_scores), key=lambda x: x[1], reverse=True)\n",
        "   final_words = copy.deepcopy(words)\n",
        " \n",
        "   # Iterate through words in decreasing importance score order\n",
        "   for top_index in list_of_index:\n",
        "       # If number of changes exceeds 40% of the whole sequence, return\n",
        "       if feature.change > int(0.4 * (len(words))): # 0.4 is the perturbation percentage\n",
        "           feature.success = 1  # exceed\n",
        "           features.append(copy.deepcopy(feature))\n",
        "           return features\n",
        " \n",
        "       # tgt_word is the word to replace\n",
        "       tgt_word = words[top_index[0]]\n",
        "       if tgt_word in filter_words:\n",
        "           continue\n",
        "       if keys[top_index[0]][0] > max_length - 2:\n",
        "           continue\n",
        " \n",
        "       # For each of L tokens in the word, top k predictions\n",
        "       substitutes = word_predictions[keys[top_index[0]][0]:keys[top_index[0]][1]]  # L, k\n",
        "       word_pred_scores = word_pred_scores_all[keys[top_index[0]][0]:keys[top_index[0]][1]]\n",
        "       # Getting most likely substitutes based on bpe\n",
        "       substitutes = get_substitues(substitutes, tokenizer_mlm, mlm_model, use_bpe, word_pred_scores, threshold_pred_score)\n",
        " \n",
        " \n",
        "       most_gap = 0.0\n",
        "       candidate = None\n",
        "       for substitute_ in substitutes:\n",
        "           substitute = substitute_\n",
        " \n",
        "           if substitute == tgt_word:\n",
        "               continue  # filter out original word\n",
        "           if '##' in substitute:\n",
        "               continue  # filter out sub-word\n",
        " \n",
        "           if substitute in filter_words:\n",
        "               continue\n",
        "           if substitute in w2i and tgt_word in w2i:\n",
        "               if cos_mat[w2i[substitute]][w2i[tgt_word]] < 0.4:\n",
        "                   continue\n",
        " \n",
        " \n",
        "           temp_replace = final_words\n",
        "           # Replace word with with substitute\n",
        "           temp_replace[top_index[0]] = substitute\n",
        "           # Convert back to text\n",
        "           temp_text = tokenizer_tgt.convert_tokens_to_string(temp_replace)\n",
        "           # Tokenize with target model tokenizer\n",
        "           inputs = tokenizer_tgt.encode_plus(temp_text, None, add_special_tokens=True, max_length=max_length, )\n",
        "           # Feed into target model\n",
        "           input_ids = torch.tensor(inputs[\"input_ids\"]).unsqueeze(0).to(device)\n",
        "           seq_len = input_ids.size(1)\n",
        "           temp_prob = tgt_model(input_ids)[0].squeeze()\n",
        "           # Increment query\n",
        "           feature.query += 1\n",
        "           # Get output label\n",
        "           temp_prob = torch.softmax(temp_prob, -1)\n",
        "           temp_label = torch.argmax(temp_prob)\n",
        " \n",
        "           # Criteria\n",
        "           original_attribution, perturbed_attribution = calculate_IGs(IG_calc_attribution, feature.seq, temp_text)\n",
        "           # Filter out different shape ones! # if original_attribution.shape == perturbed_attribution.shape:\n",
        "           similarity = cosine_similarity(original_attribution.flatten(), perturbed_attribution.flatten(), dim=0)\n",
        " \n",
        "           if temp_label != orig_label:\n",
        "               # If succeeded, increment change\n",
        "               feature.change += 1\n",
        "               # Make the change\n",
        "               final_words[top_index[0]] = substitute\n",
        "               # Append the actual changes to changes\n",
        "               feature.changes.append([keys[top_index[0]][0], substitute, tgt_word])\n",
        "               # Store the adversarial example text in final_adverse\n",
        "               feature.final_adverse = temp_text\n",
        "               feature.success = 4\n",
        "               features.append(copy.deepcopy(feature))\n",
        "           else:\n",
        " \n",
        "               label_prob = temp_prob[orig_label]\n",
        "               # Look at gap between the probabilities\n",
        "               gap = current_prob - label_prob\n",
        "               # Store the example with the most gap as candidate\n",
        "               if gap > most_gap:\n",
        "                   most_gap = gap\n",
        "                   candidate = substitute\n",
        "               feature.final_adverse = temp_text\n",
        "               features.append(copy.deepcopy(feature))\n",
        " \n",
        "       if most_gap > 0:\n",
        "           feature.change += 1\n",
        "           feature.changes.append([keys[top_index[0]][0], candidate, tgt_word])\n",
        "           current_prob = current_prob - most_gap\n",
        "           final_words[top_index[0]] = candidate\n",
        " \n",
        "   feature.final_adverse = (tokenizer_tgt.convert_tokens_to_string(final_words))\n",
        "   feature.success = 2\n",
        "   features.append(copy.deepcopy(feature))\n",
        "   # Filter out adversarial examples with changed number of tokens\n",
        "   features = [feature for feature in features if num_tokens(feature.final_adverse) == num_tokens_original]\n",
        "   return features\n"
      ],
      "metadata": {
        "id": "biupWOto8iUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DUvDP4cW96U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Load data"
      ],
      "metadata": {
        "id": "JE8CS-xSO5sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_df(df):\n",
        "  df['text'] = df['text'].apply(lambda x: re.sub(r'http[^\\s]*', '', x))\n",
        "  df['text'] = df['text'].apply(lambda x: re.sub('@[^\\s]+','', x))\n",
        "  df['text'] = df['text'].apply(lambda x: re.sub(r'#', '', x))\n",
        "  df['text'] = df['text'].apply(lambda x: re.sub(r'-', '', x))\n",
        "  df['text'] = df['text'].apply(lambda x: x.strip())\n",
        "  return df"
      ],
      "metadata": {
        "id": "UWoxockd7_dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_cls(data_path):\n",
        "    df = pd.read_csv(data_path)\n",
        "    df = preprocess_df(df)\n",
        "    features = []\n",
        "    seqs = list(df['text'])\n",
        "    labels = list(df['sentiment'])\n",
        "    features = list(zip(seqs, labels))\n",
        "    return features \n",
        "    \n",
        "features = get_data_cls(data_path)\n",
        "print(features[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKww6eBd99M1",
        "outputId": "0777539f-558b-4b36-a017-ec0138ca99f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(\"Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\", 0), (\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", 0), ('I dived many times for the ball. Managed to save 50%  The rest go out of bounds', 0), ('my whole body feels itchy and like its on fire', 0), (\"no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.\", 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Generate adversarial examples"
      ],
      "metadata": {
        "id": "3erDYz15O84h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if use_sim_mat == 1:\n",
        "    cos_mat, w2i, i2w = get_sim_embed('data_defense/counter-fitted-vectors.txt', 'data_defense/cos_sim_counter_fitting.npy')\n",
        "else:\n",
        "    cos_mat, w2i, i2w = None, {}, {}\n",
        "\n",
        "features_output = []\n",
        "\n",
        "# old_time = time.time()\n",
        "\n",
        "start = 0 \n",
        "end = 50\n",
        "outputs = []\n",
        "\n",
        "for index, feature in enumerate(tqdm(features[start:end])):\n",
        "    seq_a, label = feature\n",
        "    feat = Feature(seq_a, label, index + start)\n",
        "\n",
        "    # print('\\r number {:d} '.format(index) + tgt_path, end='')\n",
        "    feature = attack(feat, tgt_model, mlm_model, tokenizer_tgt, tokenizer_mlm, k, batch_size=32, max_length=128,\n",
        "                  cos_mat=cos_mat, w2i=w2i, i2w=i2w, use_bpe=use_bpe,threshold_pred_score=threshold_pred_score)\n",
        "\n",
        "    if feature:\n",
        "        outputs.append({'label': feature.label,\n",
        "                        'prediction': feature.pred,\n",
        "                        'success': feature.success,\n",
        "                        'change': feature.change,\n",
        "                        'num_word': len(feature.seq.split(' ')),\n",
        "                        'query': feature.query,\n",
        "                        'changes': feature.changes,\n",
        "                        'seq_a': feature.seq,\n",
        "                        'adv': feature.final_adverse,\n",
        "                        })\n",
        "        \n",
        "    if index % 100 == 0:\n",
        "      df = pd.DataFrame(outputs)\n",
        "      df.to_csv('adversarial_examples_cosine.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq4-IR9OoZYf",
        "outputId": "114a5d66-5a73-4335-b30a-a88a2dd24aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "100%|██████████| 5000/5000 [1:06:26<00:00,  1.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Save generated examples to csv"
      ],
      "metadata": {
        "id": "KATgCU3lPHjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"perturbations.csv\"\n",
        "\n",
        "def dump_features(features):\n",
        "    outputs = []\n",
        "\n",
        "    for feature in features:\n",
        "        if feature:\n",
        "            outputs.append({'label': feature.label,\n",
        "                            'prediction': feature.pred,\n",
        "                            'success': feature.success,\n",
        "                            'change': feature.change,\n",
        "                            'num_word': len(feature.seq.split(' ')),\n",
        "                            'query': feature.query,\n",
        "                            'changes': feature.changes,\n",
        "                            'seq_a': feature.seq,\n",
        "                            'adv': feature.final_adverse,\n",
        "                            })\n",
        "\n",
        "    df = pd.DataFrame(outputs)\n",
        "    df.to_csv(output_dir)\n",
        "    print('finished dump')\n",
        "    return df\n",
        "\n",
        "df = dump_features(features_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usqk7nzR8iav",
        "outputId": "27c3e493-0e93-4910-df8d-4b0a5e01ac13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished dump\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZQ8-Em0H0xHR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}