{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "find_distance_and_threshold.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkpLh5HX2Rkh",
        "outputId": "82053aca-dc6d-46c1-e293-5bca3193ba26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/Shareddrives/329T/Project/"
      ],
      "metadata": {
        "id": "ZamgC_7c8asE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd16e6a-9c5c-41a9-add0-7818feb0b008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/Shareddrives/329T/Project/'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/329T Project/"
      ],
      "metadata": {
        "id": "gauD3Dazsxjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf3c61f-742d-42d6-8427-b83e2a0ed7a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/329T Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages \n",
        "!pip3 install transformers \n",
        "!pip3 install torch"
      ],
      "metadata": {
        "id": "01GaEkoQ1BiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8857fd41-1687-4847-c2c0-017e0aa48317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from transformers import BertConfig, BertTokenizer, AutoConfig, AutoTokenizer, DistilBertConfig, DistilBertTokenizer\n",
        "from transformers import BertForSequenceClassification, BertForMaskedLM, AutoModelForSequenceClassification, DistilBertForMaskedLM\n",
        "import copy\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import copy"
      ],
      "metadata": {
        "id": "LGIKzlS78h5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import importlib\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "def install_if_not_installed(packages):\n",
        "  \"\"\"Install the given packages if they are not already installed.\"\"\"\n",
        "\n",
        "  for package in packages:\n",
        "    if isinstance(package, tuple):\n",
        "      package_name, package_package = package\n",
        "    else:\n",
        "      package_name = package\n",
        "      package_package = package\n",
        "\n",
        "    print(f\"{package_name} ... \", end='')\n",
        "\n",
        "    try:\n",
        "      importlib.import_module(package_name)\n",
        "      print(\"already installed\")\n",
        "\n",
        "    except:\n",
        "      print(f\"installing from {package_package}\")\n",
        "      subprocess.check_call(\n",
        "          [sys.executable, \"-m\", \"pip\", \"install\", package_package]\n",
        "      )\n",
        "\n",
        "install_if_not_installed(\n",
        "    [(\"trulens\", \"git+https://github.com/truera/trulens.git@piotrm/vis/output-detect\"),\n",
        "     \"pandas\",\n",
        "     \"numpy\",\n",
        "     \"domonic\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLiD5-poXfCD",
        "outputId": "fed91541-c0b3-4120-8c26-edff82298ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trulens ... already installed\n",
            "pandas ... already installed\n",
            "numpy ... already installed\n",
            "domonic ... already installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trulens.nn.attribution import IntegratedGradients\n",
        "from trulens.nn.models import get_model_wrapper\n",
        "from trulens.nn.attribution import Cut, OutputCut\n",
        "from trulens.nn.quantities import MaxClassQoI, ClassQoI, ComparativeQoI\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from trulens.visualizations import NLP"
      ],
      "metadata": {
        "id": "gd1D8XVxXhVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Args"
      ],
      "metadata": {
        "id": "eZRnaWPQvhIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "data_path = \"tweet_sentiment_pairs.csv\"\n",
        "mlm_path = \"distilbert-base-uncased\"\n",
        "tgt_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "output_dir = \"perturbations.csv\"\n",
        "use_sim_mat = 0 \n",
        "start = 0\n",
        "end = 100\n",
        "num_label = 2 \n",
        "use_bpe = 1\n",
        "k = 5\n",
        "threshold_pred_score = 0"
      ],
      "metadata": {
        "id": "VHPlomgMX10w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Initialize models"
      ],
      "metadata": {
        "id": "WhZT2vdKvjWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_mlm = DistilBertTokenizer.from_pretrained(mlm_path, do_lower_case=True)\n",
        "tokenizer_tgt = AutoTokenizer.from_pretrained(tgt_path, do_lower_case=True)\n",
        "\n",
        "config_atk = DistilBertConfig.from_pretrained(mlm_path)\n",
        "mlm_model = DistilBertForMaskedLM.from_pretrained(mlm_path, config=config_atk)\n",
        "mlm_model.to(device)\n",
        "\n",
        "config_tgt = AutoConfig.from_pretrained(tgt_path, num_labels=num_label)\n",
        "tgt_model = AutoModelForSequenceClassification.from_pretrained(tgt_path, config=config_tgt)\n",
        "tgt_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGoSlqSPXpyQ",
        "outputId": "5f6d1693-e059-41c3-e8c2-2976454dd24d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For pytorch, input_shape is required... I just used the tokenizer max_length here\n",
        "wrapped_model = get_model_wrapper(tgt_model, input_shape=(None, 128), device=device)"
      ],
      "metadata": {
        "id": "kSKYjWupXpP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43bd652-8267-45ae-df09-8e78e375f674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: get_model_wrapper: input_shape parameter is no longer used and will be removed in the future\n",
            "INFO: Detected pytorch backend for <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'>.\n",
            "INFO: Using backend Backend.PYTORCH.\n",
            "INFO: If this seems incorrect, you can force the correct backend by passing the `backend` parameter directly into your get_model_wrapper call.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integrated Gradients"
      ],
      "metadata": {
        "id": "x-z1qKnavnk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(inputs):\n",
        "    return tokenizer_tgt(inputs, padding=True, return_tensors=\"pt\").to(device)\n",
        "    # pt refers to pytorch tensor"
      ],
      "metadata": {
        "id": "Bm8bZS8MZL-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline \n",
        "from trulens.utils.nlp import token_baseline\n",
        "\n",
        "inputs_baseline_ids, inputs_baseline_embeddings = token_baseline(\n",
        "    keep_tokens=set([tokenizer_tgt.cls_token_id, tokenizer_tgt.sep_token_id]),\n",
        "    # Which tokens to preserve.\n",
        "\n",
        "    replacement_token=tokenizer_tgt.pad_token_id,\n",
        "    # What to replace tokens with.\n",
        "\n",
        "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
        "\n",
        "    ids_to_embeddings=tgt_model.get_input_embeddings()\n",
        "    # Callable to produce embeddings from token ids.\n",
        ")\n",
        "\n",
        "\n",
        "# Provisional baseline for calling .attributions as opposed to visualizing with NLP object\n",
        "inputs_baseline_ids_attribution, inputs_baseline_embeddings_attribution = token_baseline(\n",
        "    keep_tokens=set([tokenizer_tgt.cls_token_id, tokenizer_tgt.sep_token_id]),\n",
        "    # Which tokens to preserve.\n",
        "\n",
        "    replacement_token=tokenizer_tgt.pad_token_id,\n",
        "    # What to replace tokens with.\n",
        "\n",
        "    input_accessor=lambda x: x.args[0],\n",
        "\n",
        "    ids_to_embeddings=tgt_model.get_input_embeddings()\n",
        "    # Callable to produce embeddings from token ids.\n",
        ")"
      ],
      "metadata": {
        "id": "Gnthw-7RX6Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_metrics(x,y):\n",
        "    assert len(x.shape) == 1\n",
        "    assert len(y.shape) == 1\n",
        "    diff = x - y\n",
        "    return {\n",
        "        'L_1': torch.norm(diff, p=1).item(),\n",
        "        'L_2': torch.norm(diff, p=2).item(),\n",
        "        'L_inf': torch.max(torch.abs(diff)).item(),\n",
        "        'cosine_sim': cosine_similarity(x,y,dim=0).item()\n",
        "    }\n",
        "\n",
        "IG_calc = IntegratedGradients(\n",
        "    wrapped_model, \n",
        "    baseline=inputs_baseline_embeddings, \n",
        "    resolution=50,\n",
        "    doi_cut=Cut('distilbert_embeddings_word_embeddings'), \n",
        "    qoi=ComparativeQoI(1, 0), #qoi='max',\n",
        "    qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n",
        ")\n",
        "\n",
        "IG_calc_attribution = IntegratedGradients(\n",
        "    wrapped_model, \n",
        "    baseline=inputs_baseline_embeddings_attribution, \n",
        "    resolution=50,\n",
        "    doi_cut=Cut('distilbert_embeddings_word_embeddings'), \n",
        "    qoi=ComparativeQoI(1, 0), #qoi='max',  \n",
        "    qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n",
        ")"
      ],
      "metadata": {
        "id": "K_f3EmyWX_Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input:  integrated gradient calculator: (trulens.nn.attribution.IntegratedGradients())\n",
        "        original text: (str)\n",
        "        perturbed text: (str)\n",
        "Output: attribution for original text: (tensor of shape (?))\n",
        "        attribution for perturbed text: (tensor of shape (?))\n",
        "Function:\n",
        "        encode raw text of an original-adversarial pair into input tokens for the distilbert model\n",
        "        feed through IG\n",
        "        compute metrics\n",
        "'''\n",
        "def calculate_IGs(IG, string):\n",
        "    encode_string = tokenize(string)\n",
        "    input_ids = torch.tensor(encode_string[\"input_ids\"])\n",
        "    attribution = IG.attributions(input_ids.to(device))\n",
        "    \n",
        "    return attribution"
      ],
      "metadata": {
        "id": "pBlMG8ZjYA73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UCuvhhhBYDSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "plfxogLY1t3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding a suitable distance + threshold"
      ],
      "metadata": {
        "id": "0vWtECOiP_gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perturbed_data = pd.read_csv('enriched_data.csv')"
      ],
      "metadata": {
        "id": "me2p_mEf2X-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_original = perturbed_data['seq_a'][0]\n",
        "test_adversary = perturbed_data['adv'][0]\n",
        "\n",
        "explanations = calculate_IGs(IG_calc_attribution, test_original, test_adversary)\n",
        "print(test_original)\n",
        "print(test_adversary)\n",
        "print(explanations[0].shape, explanations[1].shape)\n",
        "if explanations[0].shape == explanations[1].shape:\n",
        "    metrics = similarity_metrics(explanations[0].flatten(), explanations[1].flatten())\n",
        "    print(\"Metrics:\", metrics)"
      ],
      "metadata": {
        "id": "b8EplPFLQiwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c8e07f6-f2ae-46e8-d65b-cfbdb27f252b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
            "awww, that's a bummer.  you shoulda got david green of second morning to do it. ;d\n",
            "torch.Size([1, 28, 768]) torch.Size([1, 28, 768])\n",
            "Metrics: {'L_1': 30.263, 'L_2': 0.439, 'L_inf': 0.058, 'cosine_sim': 0.928}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trulens.visualizations import NLP\n",
        "\n",
        "V = NLP(\n",
        "    wrapper=wrapped_model,\n",
        "    labels=['negative', 'positive'],\n",
        "    decode=lambda x: tokenizer_tgt.decode(x),\n",
        "    tokenize=tokenize,\n",
        "    # huggingface models can take as input the keyword args as per produced by their tokenizers.\n",
        "\n",
        "    input_accessor=lambda x: x['input_ids'],\n",
        "    # for huggingface models, input/token ids are under input_ids key in the input dictionary\n",
        "\n",
        "    output_accessor=lambda x: x['logits'],\n",
        "    # and logits under 'logits' key in the output dictionary\n",
        "\n",
        "    hidden_tokens=set([tokenizer_tgt.pad_token_id])\n",
        "    # do not display these tokens\n",
        ")\n",
        "\n",
        "# display(\n",
        "#     V.tokens(sentences, attributor=IG_calc)\n",
        "# )"
      ],
      "metadata": {
        "id": "a_oBLRT5_Cb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pairs = list(zip(list(perturbed_data['seq_a']), list(perturbed_data['adv'])))"
      ],
      "metadata": {
        "id": "mLKBdsG_Z5vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_attributions = []\n",
        "perturbed_attributions = []\n",
        "L1s = []\n",
        "L2s = []\n",
        "Linfs = []\n",
        "cos_sims = []\n",
        "\n",
        "for i in range(len(pairs)):\n",
        "    pair = pairs[i]\n",
        "    original_attribution = calculate_IGs(IG_calc_attribution, pair[0])\n",
        "    perturbed_attribution = calculate_IGs(IG_calc_attribution, pair[1])\n",
        "    if original_attribution.shape == perturbed_attribution.shape:\n",
        "        print(\"--------------------------------\")\n",
        "        print(f\"Iteration: {i}\")\n",
        "        print(\"Original: \", pair[0])\n",
        "        print(\"Adversary: \", pair[1])\n",
        "        orig_attr_summed = torch.sum(original_attribution, -1).flatten().cpu()\n",
        "        pert_attr_summed = torch.sum(perturbed_attribution, -1).flatten().cpu()\n",
        "        original_attributions.append(orig_attr_summed.cpu())\n",
        "        perturbed_attributions.append(pert_attr_summed.cpu())\n",
        "        # print(orig_attr_summed)\n",
        "        # print(pert_attr_summed)\n",
        "\n",
        "        # metrics = similarity_metrics(orig_attr_summed, pert_attr_summed)\n",
        "        # L1s.append(metrics[\"L_1\"])\n",
        "        # L2s.append(metrics[\"L_2\"])\n",
        "        # Linfs.append(metrics[\"L_inf\"])\n",
        "        # cos_sims.append(metrics[\"cosine_sim\"])\n",
        "        display(V.tokens(list(pair), attributor=IG_calc))\n",
        "        print(\"--------------------------------\")\n",
        "        \n",
        "        # print(\"Metrics:\", metrics)\n",
        "    else:\n",
        "        \n",
        "        print(f\"Token number mismatch on pair {i}: {original_attribution.shape} != {perturbed_attribution.shape}\")\n",
        "        assert 1 == 0\n"
      ],
      "metadata": {
        "id": "VEjmVYUxPSEN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e68cd9aa-b604-44b1-b1e2-d7e9141a19ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------\n",
            "Iteration: 0\n",
            "Original:  Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
            "Adversary:  awww, that's a bummer.  you shoulda got david green of second morning to do it. ;d\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 1\n",
            "Original:  Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
            "Adversary:  awww, that's a bummer.  you shoulda gotten david green of second morning to doing it. ;d\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 2\n",
            "Original:  is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
            "Adversary:  is worried that he can't access his facebook by texting it... and might cry as a result  school today also. blah!\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 3\n",
            "Original:  is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\n",
            "Adversary:  is worried that he can't access his twitter by texting it... and might cry as a result  school today also. blah!\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 4\n",
            "Original:  I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n",
            "Adversary:  i dived many times for the ball. got to catch 50%  the rest went out of bounds\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 5\n",
            "Original:  I dived many times for the ball. Managed to save 50%  The rest go out of bounds\n",
            "Adversary:  i dived several times for the ball. got to catch 50%  the rest went out of bounds\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 6\n",
            "Original:  my whole body feels itchy and like its on fire\n",
            "Adversary:  my whole skin feels itchy and like its on fire\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 7\n",
            "Original:  my whole body feels itchy and like its on fire\n",
            "Adversary:  my whole skin feel itchy and like its on fire\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 8\n",
            "Original:  hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\n",
            "Adversary:  hey  long way no see! yes.. hey a bit,only a bit  lol, i'm okay hey, how's you?\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 9\n",
            "Original:  hey  long time no see! Yes.. Rains a bit ,only a bit  LOL , I'm fine thanks , how's you ?\n",
            "Adversary:  hey  long way no see! yes.. rains a bit,only a bit  lol, i'm okay hey, how's you?\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 10\n",
            "Original:  I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .\n",
            "Adversary:  i couldn't stand to see it.  and i thought the financial lost was humiliating.....\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 11\n",
            "Original:  I couldn't bear to watch it.  And I thought the UA loss was embarrassing . . . . .\n",
            "Adversary:  i couldn't stand to see it.  and i think the financial lost was humiliating.....\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 12\n",
            "Original:  It it counts, idk why I did either. you never talk to me anymore\n",
            "Adversary:  it it counts, idk why i do either. you never talk to me anymore\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 13\n",
            "Original:  It it counts, idk why I did either. you never talk to me anymore\n",
            "Adversary:  it it counts, idk why i do either. you never speak to me anymore\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 14\n",
            "Original:  i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.\n",
            "Adversary:  i would've been the first, but i didn't have a gun.    not really though, josh snyder's just a doucheclown.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 15\n",
            "Original:  i would've been the first, but i didn't have a gun.    not really though, zac snyder's just a doucheclown.\n",
            "Adversary:  i would've been the first, but i didn't carry a gun.    not really though, josh snyder's just a doucheclown.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 16\n",
            "Original:  I wish I got to watch it with you!! I miss you and   how was the premiere?!\n",
            "Adversary:  i wish i got to see it with you!! i miss you and   how was the premiere?!\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 17\n",
            "Original:  I wish I got to watch it with you!! I miss you and   how was the premiere?!\n",
            "Adversary:  i wished i had to see it with you!! i missed you and   how was the premiere?!\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 18\n",
            "Original:  Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?\n",
            "Adversary:  hollis' death scenes will hit me severely to watch on film ? is director cuts not out now?\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 19\n",
            "Original:  Hollis' death scene will hurt me severely to watch on film  wry is directors cut not out now?\n",
            "Adversary:  hollis' death scenes will hit me severely to watch on film  wry is director cuts not out now?\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 20\n",
            "Original:  ahh ive always wanted to see rent  love the soundtrack!!\n",
            "Adversary:  ahh ive always want to watch rent  love the soundtrack!!\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 21\n",
            "Original:  ahh ive always wanted to see rent  love the soundtrack!!\n",
            "Adversary:  ahh ive always wanted to watch rent  love the soundtrack!!\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 22\n",
            "Original:  Oh dear. Were you drinking out of the forgotten table drinks?\n",
            "Adversary:  oh dear. were you drinking out of the forgot room drinks?\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 23\n",
            "Original:  Oh dear. Were you drinking out of the forgotten table drinks?\n",
            "Adversary:  oh dear. were you having out of the forgot room drinks?\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 24\n",
            "Original:  i was out most of the day so didn't get much done\n",
            "Adversary:  i was out most of the day so didn't got much done\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 25\n",
            "Original:  i was out most of the day so didn't get much done\n",
            "Adversary:  i was out most of the morning so didn't got much done\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 26\n",
            "Original:  one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh*\n",
            "Adversary:  one of my friend asked me, and called to meet with her at mid valley today...but i've no way *sigh*\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 27\n",
            "Original:  one of my friend called me, and asked to meet with her at Mid Valley today...but i've no time *sigh*\n",
            "Adversary:  one of my friend called me, and called to meet with her at mid valley today...but i've no way *sigh*\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 28\n",
            "Original:  I baked you a cake but I ated it\n",
            "Adversary:  i made you a cakes but i ated it\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 29\n",
            "Original:  I baked you a cake but I ated it\n",
            "Adversary:  i made you a cake but i ated it\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 30\n",
            "Original:  this week is not going as i had hoped\n",
            "Adversary:  this week is not go as i had hoped\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 31\n",
            "Original:  this week is not going as i had hoped\n",
            "Adversary:  this day is not go as i had hoped\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 32\n",
            "Original:  blagh class at 8 tomorrow\n",
            "Adversary:  blagh class at 8 tomorrow\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 33\n",
            "Original:  blagh class at 8 tomorrow\n",
            "Adversary:  blagh classes at 9 tomorrow\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 34\n",
            "Original:  I hate when I have to call and wake people up\n",
            "Adversary:  i hated when i need to phone and woke someone up\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 35\n",
            "Original:  I hate when I have to call and wake people up\n",
            "Adversary:  i hated when i have to call and wake people up\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 36\n",
            "Original:  Just going to cry myself to sleep after watching Marley and Me.\n",
            "Adversary:  just going to cry myself to dream after watching marley and me.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 37\n",
            "Original:  Just going to cry myself to sleep after watching Marley and Me.\n",
            "Adversary:  just going to cry myself to dream after watch marley and me.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 38\n",
            "Original:  ooooh.... LOL  that leslie.... and ok I won't do it again so leslie won't  get mad again\n",
            "Adversary:  ooooh.... lol  that leslie.... and alright i won't do it again so leslie won't  be crazy again\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 39\n",
            "Original:  ooooh.... LOL  that leslie.... and ok I won't do it again so leslie won't  get mad again\n",
            "Adversary:  ooooh.... lol  that leslie.... and ok i won't do it again so leslie won't  be crazy again\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 40\n",
            "Original:  Meh... Almost Lover is the exception... this track gets me depressed every time.\n",
            "Adversary:  meh... almost lover is the exception... this track gets me sad every time.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 41\n",
            "Original:  Meh... Almost Lover is the exception... this track gets me depressed every time.\n",
            "Adversary:  meh... almost love is the exception... this song makes me sad every time.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 42\n",
            "Original:  some1 hacked my account on aim  now i have to make a new one\n",
            "Adversary:  some1 hacking my accounts on account  now i have to create a new one\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 43\n",
            "Original:  some1 hacked my account on aim  now i have to make a new one\n",
            "Adversary:  some1 hacking my accounts on account  now i need to create a fresh one\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 44\n",
            "Original:  I want to go to promote GEAR AND GROOVE but unfornately no ride there  I may b going to the one in Anaheim in May though\n",
            "Adversary:  i want to go to promotion gears and groove but unfornately no rides there  i may be going to the one in anaheim in may though\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 45\n",
            "Original:  I want to go to promote GEAR AND GROOVE but unfornately no ride there  I may b going to the one in Anaheim in May though\n",
            "Adversary:  i want to go to promotion gear and groove but unfornately no rides there  i may be going to the one in anaheim in may though\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 46\n",
            "Original:  thought sleeping in was an option tomorrow but realizing that it now is not. evaluations in the morning and work in the afternoon!\n",
            "Adversary:  thought sleeping in was an alternative today but realized that it now is not. evaluations in the morning and work in the afternoon!\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 47\n",
            "Original:  thought sleeping in was an option tomorrow but realizing that it now is not. evaluations in the morning and work in the afternoon!\n",
            "Adversary:  thought sleeping in was an alternative tomorrow but realizing that it now is not. evaluations in the morning and work in the afternoon!\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 48\n",
            "Original:  awe i love you too!!!! 1 am here  i miss you\n",
            "Adversary:  awe i love you too!!!! 1 am here  i miss you\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 49\n",
            "Original:  awe i love you too!!!! 1 am here  i miss you\n",
            "Adversary:  ! i loved you too!!!! 1 am here  i love you\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 50\n",
            "Original:  I cry my asian eyes to sleep at night\n",
            "Adversary:  i call my asian eyes to sleep at night\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 51\n",
            "Original:  I cry my asian eyes to sleep at night\n",
            "Adversary:  i call my yellow eye to dream at night\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 52\n",
            "Original:  ok I'm sick and spent an hour sitting in the shower cause I was too sick to stand and held back the puke like a champ. BED now\n",
            "Adversary:  ok i'm ill and had an hour standing in the showers cause i was too sick to sit and held back the puke like a champ. sleep now\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 53\n",
            "Original:  ok I'm sick and spent an hour sitting in the shower cause I was too sick to stand and held back the puke like a champ. BED now\n",
            "Adversary:  ok i'm ill and had an hours standing in the showers cause i was too ill to sit and fought back the puke like a champ. sleep now\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 54\n",
            "Original:  ill tell ya the story later  not a good day and ill be workin for like three more hours...\n",
            "Adversary:  ill told ya the tale later  not a bad morning and ill be workin for like four more hours...\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 55\n",
            "Original:  ill tell ya the story later  not a good day and ill be workin for like three more hours...\n",
            "Adversary:  ill told ya the story later  not a bad day and ill be workin for like four more hours...\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 56\n",
            "Original:  I don't either. Its depressing. I don't think I even want to know about the kids in suitcases.\n",
            "Adversary:  i don't either. its depressing. i don't know i even like to tell about the children in suitcases.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 57\n",
            "Original:  I don't either. Its depressing. I don't think I even want to know about the kids in suitcases.\n",
            "Adversary:  i don't either. its depressing. i don't think i even want to tell about the kids in suitcases.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 58\n",
            "Original:  Bed. Class 812. Work 123. Gym 35 or 6. Then class 610. Another day that's gonna fly by. I miss my girlfriend\n",
            "Adversary:  bed. classes 812. working 123. gym 35 or 6. then classes 610. another day that's gonna be by. i missed my girlfriend\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 59\n",
            "Original:  Bed. Class 812. Work 123. Gym 35 or 6. Then class 610. Another day that's gonna fly by. I miss my girlfriend\n",
            "Adversary:  bed. class 812. working 123. gym 35 or 6. then class 610. another day that's gonna fly by. i missed my girlfriend\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 60\n",
            "Original:  really don't feel like getting up today... but got to study to for tomorrows practical exam...\n",
            "Adversary:  really don't feeling like get up today... but had to study to for tomorrows practical exam...\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 61\n",
            "Original:  really don't feel like getting up today... but got to study to for tomorrows practical exam...\n",
            "Adversary:  really don't feel like getting up today... but had to study to for tomorrows practical exam...\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 62\n",
            "Original:  He's the reason for the teardrops on my guitar the only one who has enough of me to break my heart\n",
            "Adversary:  he's the reasons for the teardrops on my band the only one who had enough of me to broken my heart\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 63\n",
            "Original:  He's the reason for the teardrops on my guitar the only one who has enough of me to break my heart\n",
            "Adversary:  he's the reasons for the teardrops on my band the only one who had enough of me to break my heart\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 64\n",
            "Original:  Sad, sad, sad. I don't know why but I hate this feeling  I wanna sleep and I still can't!\n",
            "Adversary:  sad, sad, sad. i don't sure why but i hated this feeling  i want dream and i really can't!\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 65\n",
            "Original:  Sad, sad, sad. I don't know why but I hate this feeling  I wanna sleep and I still can't!\n",
            "Adversary:  sad, sad, sad. i don't sure why but i hated this feel  i want dream and i really can't!\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 66\n",
            "Original:  Awww I soo wish I was there to see you finally comfortable! Im sad that I missed it\n",
            "Adversary:  awww i soo wished i was there to hear you really comfortable! im sorry that i miss it\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 67\n",
            "Original:  Awww I soo wish I was there to see you finally comfortable! Im sad that I missed it\n",
            "Adversary:  awww i soo wished i was there to hear you really comfortable! im sorry that i miss it\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 68\n",
            "Original:  Falling asleep. Just heard about that Tracy girl's body being found. How sad  My heart breaks for that family.\n",
            "Adversary:  falling asleep. just heard about that tracy girl's bodies being found. how happy  my heart breaks for that family.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 69\n",
            "Original:  Falling asleep. Just heard about that Tracy girl's body being found. How sad  My heart breaks for that family.\n",
            "Adversary:  falling asleep. just hear about that little girl's bodies being found. how happy  my love feels for that family.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 70\n",
            "Original:  Yay! I'm happy for you with your job! But that also means less time for me and you...\n",
            "Adversary:  yay! i'm happy for you with your job! but that also means less money for me and you...\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 71\n",
            "Original:  Yay! I'm happy for you with your job! But that also means less time for me and you...\n",
            "Adversary:  yay! i'm pleased for you with your job! but that also means less money for me and you...\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 72\n",
            "Original:  Just checked my user timeline on my blackberry, it looks like the twanking is still happening  Are ppl still having probs w/ BGs and UIDs?\n",
            "Adversary:  just checked my users id on my blackberry, it looks like the twanking is still happening  are ppl still having probs w/ bgs and uids?\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 73\n",
            "Original:  Just checked my user timeline on my blackberry, it looks like the twanking is still happening  Are ppl still having probs w/ BGs and UIDs?\n",
            "Adversary:  just checked my users id on my blackberry, it seems like the twanking is still happening  are ppl still having probs w/ bgs and uids?\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 74\n",
            "Original:  Oh man...was ironing  fave top to wear to a meeting. Burnt it\n",
            "Adversary:  oh man...was ironing  fave top to tie to a meeting. burned it\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 75\n",
            "Original:  Oh man...was ironing  fave top to wear to a meeting. Burnt it\n",
            "Adversary:  oh man...was ironing  fave top to tie to a meeting. burned it\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 76\n",
            "Original:  is strangely sad about LiLo and SamRo breaking up.\n",
            "Adversary:  is oddly happy about lilo and samro falling up.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 77\n",
            "Original:  is strangely sad about LiLo and SamRo breaking up.\n",
            "Adversary:  is strangely happy about lilo and samro breaking up.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 78\n",
            "Original:  oh! i'm so sorry  i didn't think about that before retweeting.\n",
            "Adversary:  oh! i'm really apologize  i didn't thought about that before retweeting.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 79\n",
            "Original:  oh! i'm so sorry  i didn't think about that before retweeting.\n",
            "Adversary:  oh! i'm really sorry  i didn't think about that before retweeting.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 80\n",
            "Original:  Broadband plan 'a massive broken promise'  via www.diigo.com/~tautao Still waiting for broadband we are\n",
            "Adversary:  broadband plans 'a massive broken promise'  via www.diigo.com/~tautao still awaiting for broadband we are\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 81\n",
            "Original:  Broadband plan 'a massive broken promise'  via www.diigo.com/~tautao Still waiting for broadband we are\n",
            "Adversary:  broadband plan 'a massive broken promise'  via www.diigo.com/~tautao still awaiting for broadband we are\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 82\n",
            "Original:  Wow, tons of replies from you, may have to unfollow so I can see my friends' tweets, you're scrolling the feed a lot.\n",
            "Adversary:  wow, lots of replies from you, may need to unfollow so i can see my friends' tweets, you're watching the feeds a lot.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 83\n",
            "Original:  Wow, tons of replies from you, may have to unfollow so I can see my friends' tweets, you're scrolling the feed a lot.\n",
            "Adversary:  wow, lots of responses from you, may need to unfollow so i can read my friends' tweets, you're watching the feeds a lot.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 84\n",
            "Original:  our duck and chicken are taking wayyy too long to hatch\n",
            "Adversary:  our duck and chickens are take wayyy too longer to hatch\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 85\n",
            "Original:  our duck and chicken are taking wayyy too long to hatch\n",
            "Adversary:  our ducks and chickens are take wayyy too longer to hatch\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 86\n",
            "Original:  Put vacation photos online a few yrs ago. PC crashed, and now I forget the name of the site.\n",
            "Adversary:  put holiday pictures online a few yrs ago. pc crashed, and now i remember the names of the site.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 87\n",
            "Original:  Put vacation photos online a few yrs ago. PC crashed, and now I forget the name of the site.\n",
            "Adversary:  put holiday pictures online a few yrs ago. pc crashed, and now i forget the names of the site.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 88\n",
            "Original:  I need a hug\n",
            "Adversary:  i want a hug\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 89\n",
            "Original:  I need a hug\n",
            "Adversary:  i want a hug\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 90\n",
            "Original:  Not sure what they are, only that they are PoS! As much as I want to, I dont think can trade away company assets sorry andy!\n",
            "Adversary:  not know what they are, only that they are pos! as badly as i like to, i dont know can trade away company assets sorry andy!\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 91\n",
            "Original:  Not sure what they are, only that they are PoS! As much as I want to, I dont think can trade away company assets sorry andy!\n",
            "Adversary:  not know what they are, only that they are pos! as badly as i like to, i dont know can trade away company funds sorry andy!\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 92\n",
            "Original:  I have a sad feeling that Dallas is not going to show up  I gotta say though, you'd think more shows would use music from the game. mmm\n",
            "Adversary:  i know a sadness feel that texas is not gonna to showing up  i gotta think though, you'd know more show would have song from the game. mmm\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 93\n",
            "Original:  I have a sad feeling that Dallas is not going to show up  I gotta say though, you'd think more shows would use music from the game. mmm\n",
            "Adversary:  i have a sad feeling that dallas is not gonna to showing up  i gotta think though, you'd think more shows would have music from the game. mmm\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 94\n",
            "Original:  Where did u move to?  I thought u were already in sd. ?? Hmmm. Random u found me. Glad to hear yer doing well.\n",
            "Adversary:  Where did u move to?  I thought u were already in sd. ?? Hmmm. Random u found me. Glad to hear yer doing well.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 95\n",
            "Original:  Where did u move to?  I thought u were already in sd. ?? Hmmm. Random u found me. Glad to hear yer doing well.\n",
            "Adversary:  where did u move to?  i thought u were already in sd.?? hmmm. random u find me. good to see yer doing well.\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 96\n",
            "Original:  I miss my ps3, it's out of commission  Wutcha playing? Have you copped 'Blood On The Sand'?\n",
            "Adversary:  i missed my ps3, it's out of play  wutcha playing? had you copped 'blood on the sand'?\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 97\n",
            "Original:  I miss my ps3, it's out of commission  Wutcha playing? Have you copped 'Blood On The Sand'?\n",
            "Adversary:  i missed my ps3, it's out of play  wutcha playing? have you copped 'blood on the sand'?\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 98\n",
            "Original:  just leaving the parking lot of work!\n",
            "Adversary:  just leave the parking lot of work!\n",
            "--------------------------------\n",
            "--------------------------------\n",
            "Iteration: 99\n",
            "Original:  just leaving the parking lot of work!\n",
            "Adversary:  just leave the park lot of work!\n",
            "--------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perturbed_data['original_attribution'] = original_attributions\n",
        "perturbed_data['perturbed_attribution'] = perturbed_attributions\n",
        "perturbed_data['L1'] = L1s\n",
        "perturbed_data['L2'] = L2s\n",
        "perturbed_data['Linf'] = Linfs\n",
        "perturbed_data['cosine_similarity'] = cos_sims"
      ],
      "metadata": {
        "id": "GsE4Rjag7_v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [0,0,0,0,1,1,0,0,1,1,1,1,0,1,0,0,0,1,0,1,0,0,1,1,0,0,1,1,0,0,0,0,0,0,0,0,1,1,1,1,1,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,1,1,1,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,1,0,0,0,1,1,0,0,0,0,1,1,1,1,1,1,0,0,1,1,0,0]"
      ],
      "metadata": {
        "id": "Z5qzJ3JoZijR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perturbed_data = perturbed_data[:100]\n",
        "perturbed_data['robust'] = labels"
      ],
      "metadata": {
        "id": "N0xJtUrLZi0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame = pd.read_csv('enriched_data.csv')"
      ],
      "metadata": {
        "id": "sp6sypc2YyQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_metrics(data, metric, threshold):\n",
        "    # Count for confusion matrix\n",
        "    if metric == 'cosine_similarity':\n",
        "        # Need special case for cosine similarity since smaller value implies greater difference\n",
        "        TP = len(data.query(f\"{metric} < {threshold} & robust == 1\"))\n",
        "        FN = len(data.query(f\"{metric} >= {threshold} & robust == 1\"))\n",
        "        FP = len(data.query(f\"{metric} < {threshold} & robust == 0\"))\n",
        "        TN = len(data.query(f\"{metric} >= {threshold} & robust == 0\"))\n",
        "    else:\n",
        "        TP = len(data.query(f\"{metric} > {threshold} & robust == 1\"))\n",
        "        FN = len(data.query(f\"{metric} <= {threshold} & robust == 1\"))\n",
        "        FP = len(data.query(f\"{metric} > {threshold} & robust == 0\"))\n",
        "        TN = len(data.query(f\"{metric} <= {threshold} & robust == 0\"))\n",
        "    # Ensure correct counts\n",
        "    assert TP + TN + FP + FN == len(data)\n",
        "    # Collect statistics\n",
        "    precision = TP / (TP + FP)\n",
        "    recall = TP / (TP + FN)\n",
        "    F1 = 2 * precision * recall / (precision + recall)\n",
        "    accuracy = (TP + TN) / len(data)\n",
        "    return round(precision, 4), round(recall, 4), round(F1, 4), round(accuracy, 4)"
      ],
      "metadata": {
        "id": "3BYau8iVgIlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metric_and_threshold(data, similarity_metrics):\n",
        "    best_prec = [-1]\n",
        "    best_rec = [-1]\n",
        "    best_f1 = [-1]\n",
        "    best_acc = [-1]\n",
        "    for metric in similarity_metrics:\n",
        "        # Generate range for linear search using median of class\n",
        "        low = np.median(data[data['robust']==0][metric])\n",
        "        high = np.median(data[data['robust']==1][metric])\n",
        "        # Linear search with num steps\n",
        "        for threshold in np.linspace(low, high, num=50):\n",
        "            precision, recall, F1, accuracy = evaluate_metrics(data, metric, threshold)\n",
        "            # Update best performances with score, metric, and threshold\n",
        "            threshold = round(threshold, 4)\n",
        "            if precision >= best_prec[0]:\n",
        "                best_prec = (precision, metric, threshold)\n",
        "            if recall >= best_rec[0]:\n",
        "                best_rec = (recall, metric, threshold)\n",
        "            if F1 >= best_f1[0]:\n",
        "                best_f1 = (F1, metric, threshold)\n",
        "            if accuracy >= best_acc[0]:\n",
        "                best_acc = (accuracy, metric, threshold)\n",
        "    return best_prec, best_rec, best_f1, best_acc"
      ],
      "metadata": {
        "id": "i_uOCyjlkNPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = \"Linf\"\n",
        "low = np.median(frame[frame['robust']==0][metric])\n",
        "high = np.median(frame[frame['robust']==1][metric])\n",
        "print(low,high)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zh9iuC02NxQ",
        "outputId": "92898b57-cce5-44e8-f467-c45a3a0e9fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.253 0.669\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_metrics = ['L1', 'L2', 'Linf', 'cosine_similarity']\n",
        "for metric in similarity_metrics:\n",
        "    precision, recall, f1, accuracy = metric_and_threshold(frame, [metric])\n",
        "    print(metric)\n",
        "    print(f\"Best precision {precision[0]} at threshold {precision[2]}\")\n",
        "    print(f\"Best recall {recall[0]} at threshold {recall[2]}\")\n",
        "    print(f\"Best F1 {f1[0]} at threshold {f1[2]}\")\n",
        "    print(f\"Best accuracy {accuracy[0]} at threshold {accuracy[2]}\")\n",
        "    print(\"-------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnnukWcronrV",
        "outputId": "c81d8565-d9d4-4082-8946-364ac418eb2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1\n",
            "Best precision 0.7097 at threshold 4.3849\n",
            "Best recall 1.0 at threshold 2.1441\n",
            "Best F1 0.7358 at threshold 2.1441\n",
            "Best accuracy 0.76 at threshold 2.9844\n",
            "-------------------------------\n",
            "L2\n",
            "Best precision 0.6897 at threshold 1.6005\n",
            "Best recall 0.9487 at threshold 0.7235\n",
            "Best F1 0.7447 at threshold 0.7819\n",
            "Best accuracy 0.76 at threshold 0.7819\n",
            "-------------------------------\n",
            "Linf\n",
            "Best precision 0.7632 at threshold 0.5417\n",
            "Best recall 0.8718 at threshold 0.3634\n",
            "Best F1 0.7532 at threshold 0.5417\n",
            "Best accuracy 0.81 at threshold 0.5417\n",
            "-------------------------------\n",
            "cosine_similarity\n",
            "Best precision 0.725 at threshold 0.8153\n",
            "Best recall 0.9231 at threshold 0.8687\n",
            "Best F1 0.7865 at threshold 0.8628\n",
            "Best accuracy 0.81 at threshold 0.8509\n",
            "-------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision, recall, f1, accuracy = metric_and_threshold(frame, similarity_metrics)\n",
        "print(\"Overall\")\n",
        "print(f\"Best precision {precision[0]} with metric {precision[1]} at threshold {precision[2]}\")\n",
        "print(f\"Best recall {recall[0]} with metric {recall[1]} at threshold {recall[2]}\")\n",
        "print(f\"Best F1 {f1[0]} with metric {f1[1]} at threshold {f1[2]}\")\n",
        "print(f\"Best accuracy {accuracy[0]} with metric {accuracy[1]} at threshold {accuracy[2]}\")\n",
        "print(\"-------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3Nkgm0AwK9o",
        "outputId": "b6784edc-dad3-4d00-a14c-1f6f8611c51e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall\n",
            "Best precision 0.7632 with metric Linf at threshold 0.5417\n",
            "Best recall 1.0 with metric L1 at threshold 2.1441\n",
            "Best F1 0.7865 with metric cosine_similarity at threshold 0.8628\n",
            "Best accuracy 0.81 with metric cosine_similarity at threshold 0.8509\n",
            "-------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluate_metrics(frame, 'cosine_similarity', 0.8628))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5H2n7z-kNIV",
        "outputId": "c346703f-b79e-48b7-94e6-61a8e07e8c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0.7, 0.8974, 0.7865, 0.81)\n"
          ]
        }
      ]
    }
  ]
}