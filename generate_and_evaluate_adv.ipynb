{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_and_evaluate_adv.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BpRSTmSlTGv3",
        "cxh7U-kaPKaO"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkpLh5HX2Rkh",
        "outputId": "82053aca-dc6d-46c1-e293-5bca3193ba26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/Shareddrives/329T/Project/"
      ],
      "metadata": {
        "id": "ZamgC_7c8asE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd16e6a-9c5c-41a9-add0-7818feb0b008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/Shareddrives/329T/Project/'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/329T Project/"
      ],
      "metadata": {
        "id": "gauD3Dazsxjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf3c61f-742d-42d6-8427-b83e2a0ed7a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/329T Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages \n",
        "!pip3 install transformers \n",
        "!pip3 install torch"
      ],
      "metadata": {
        "id": "01GaEkoQ1BiX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8857fd41-1687-4847-c2c0-017e0aa48317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.11.0+cu113)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (4.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from transformers import BertConfig, BertTokenizer, AutoConfig, AutoTokenizer, DistilBertConfig, DistilBertTokenizer\n",
        "from transformers import BertForSequenceClassification, BertForMaskedLM, AutoModelForSequenceClassification, DistilBertForMaskedLM\n",
        "import copy\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import copy"
      ],
      "metadata": {
        "id": "LGIKzlS78h5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import importlib\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "def install_if_not_installed(packages):\n",
        "  \"\"\"Install the given packages if they are not already installed.\"\"\"\n",
        "\n",
        "  for package in packages:\n",
        "    if isinstance(package, tuple):\n",
        "      package_name, package_package = package\n",
        "    else:\n",
        "      package_name = package\n",
        "      package_package = package\n",
        "\n",
        "    print(f\"{package_name} ... \", end='')\n",
        "\n",
        "    try:\n",
        "      importlib.import_module(package_name)\n",
        "      print(\"already installed\")\n",
        "\n",
        "    except:\n",
        "      print(f\"installing from {package_package}\")\n",
        "      subprocess.check_call(\n",
        "          [sys.executable, \"-m\", \"pip\", \"install\", package_package]\n",
        "      )\n",
        "\n",
        "install_if_not_installed(\n",
        "    [(\"trulens\", \"git+https://github.com/truera/trulens.git@piotrm/vis/output-detect\"),\n",
        "     \"pandas\",\n",
        "     \"numpy\",\n",
        "     \"domonic\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLiD5-poXfCD",
        "outputId": "fed91541-c0b3-4120-8c26-edff82298ce6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trulens ... already installed\n",
            "pandas ... already installed\n",
            "numpy ... already installed\n",
            "domonic ... already installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trulens.nn.attribution import IntegratedGradients\n",
        "from trulens.nn.models import get_model_wrapper\n",
        "from trulens.nn.attribution import Cut, OutputCut\n",
        "from trulens.nn.quantities import MaxClassQoI, ClassQoI, ComparativeQoI\n",
        "from torch.nn.functional import cosine_similarity\n",
        "from trulens.visualizations import NLP"
      ],
      "metadata": {
        "id": "gd1D8XVxXhVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Args"
      ],
      "metadata": {
        "id": "eZRnaWPQvhIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "data_path = \"tweet_sentiment_pairs.csv\"\n",
        "mlm_path = \"distilbert-base-uncased\"\n",
        "tgt_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "output_dir = \"perturbations.csv\"\n",
        "use_sim_mat = 0 \n",
        "start = 0\n",
        "end = 100\n",
        "num_label = 2 \n",
        "use_bpe = 1\n",
        "k = 5\n",
        "threshold_pred_score = 0"
      ],
      "metadata": {
        "id": "VHPlomgMX10w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Initialize models"
      ],
      "metadata": {
        "id": "WhZT2vdKvjWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_mlm = DistilBertTokenizer.from_pretrained(mlm_path, do_lower_case=True)\n",
        "tokenizer_tgt = AutoTokenizer.from_pretrained(tgt_path, do_lower_case=True)\n",
        "\n",
        "config_atk = DistilBertConfig.from_pretrained(mlm_path)\n",
        "mlm_model = DistilBertForMaskedLM.from_pretrained(mlm_path, config=config_atk)\n",
        "mlm_model.to(device)\n",
        "\n",
        "config_tgt = AutoConfig.from_pretrained(tgt_path, num_labels=num_label)\n",
        "tgt_model = AutoModelForSequenceClassification.from_pretrained(tgt_path, config=config_tgt)\n",
        "tgt_model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGoSlqSPXpyQ",
        "outputId": "5f6d1693-e059-41c3-e8c2-2976454dd24d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertForSequenceClassification(\n",
              "  (distilbert): DistilBertModel(\n",
              "    (embeddings): Embeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (transformer): Transformer(\n",
              "      (layer): ModuleList(\n",
              "        (0): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (1): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (2): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (3): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (4): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "        (5): TransformerBlock(\n",
              "          (attention): MultiHeadSelfAttention(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (ffn): FFN(\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (activation): GELUActivation()\n",
              "          )\n",
              "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For pytorch, input_shape is required... I just used the tokenizer max_length here\n",
        "wrapped_model = get_model_wrapper(tgt_model, input_shape=(None, 128), device=device)"
      ],
      "metadata": {
        "id": "kSKYjWupXpP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f43bd652-8267-45ae-df09-8e78e375f674"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: get_model_wrapper: input_shape parameter is no longer used and will be removed in the future\n",
            "INFO: Detected pytorch backend for <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'>.\n",
            "INFO: Using backend Backend.PYTORCH.\n",
            "INFO: If this seems incorrect, you can force the correct backend by passing the `backend` parameter directly into your get_model_wrapper call.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Integrated Gradients"
      ],
      "metadata": {
        "id": "x-z1qKnavnk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(inputs):\n",
        "    return tokenizer_tgt(inputs, padding=True, return_tensors=\"pt\").to(device)\n",
        "    # pt refers to pytorch tensor"
      ],
      "metadata": {
        "id": "Bm8bZS8MZL-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline \n",
        "from trulens.utils.nlp import token_baseline\n",
        "\n",
        "inputs_baseline_ids, inputs_baseline_embeddings = token_baseline(\n",
        "    keep_tokens=set([tokenizer_tgt.cls_token_id, tokenizer_tgt.sep_token_id]),\n",
        "    # Which tokens to preserve.\n",
        "\n",
        "    replacement_token=tokenizer_tgt.pad_token_id,\n",
        "    # What to replace tokens with.\n",
        "\n",
        "    input_accessor=lambda x: x.kwargs['input_ids'],\n",
        "\n",
        "    ids_to_embeddings=tgt_model.get_input_embeddings()\n",
        "    # Callable to produce embeddings from token ids.\n",
        ")\n",
        "\n",
        "\n",
        "# Provisional baseline for calling .attributions as opposed to visualizing with NLP object\n",
        "inputs_baseline_ids_attribution, inputs_baseline_embeddings_attribution = token_baseline(\n",
        "    keep_tokens=set([tokenizer_tgt.cls_token_id, tokenizer_tgt.sep_token_id]),\n",
        "    # Which tokens to preserve.\n",
        "\n",
        "    replacement_token=tokenizer_tgt.pad_token_id,\n",
        "    # What to replace tokens with.\n",
        "\n",
        "    input_accessor=lambda x: x.args[0],\n",
        "\n",
        "    ids_to_embeddings=tgt_model.get_input_embeddings()\n",
        "    # Callable to produce embeddings from token ids.\n",
        ")"
      ],
      "metadata": {
        "id": "Gnthw-7RX6Oi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity_metrics(x,y):\n",
        "    assert len(x.shape) == 1\n",
        "    assert len(y.shape) == 1\n",
        "    diff = x - y\n",
        "    return {\n",
        "        'L_1': torch.norm(diff, p=1).item(),\n",
        "        'L_2': torch.norm(diff, p=2).item(),\n",
        "        'L_inf': torch.max(torch.abs(diff)).item(),\n",
        "        'cosine_sim': cosine_similarity(x,y,dim=0).item()\n",
        "    }\n",
        "\n",
        "IG_calc = IntegratedGradients(\n",
        "    wrapped_model, \n",
        "    baseline=inputs_baseline_embeddings, \n",
        "    resolution=50,\n",
        "    doi_cut=Cut('distilbert_embeddings_word_embeddings'), \n",
        "    qoi=ComparativeQoI(1, 0), #qoi='max',\n",
        "    qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n",
        ")\n",
        "\n",
        "IG_calc_attribution = IntegratedGradients(\n",
        "    wrapped_model, \n",
        "    baseline=inputs_baseline_embeddings_attribution, \n",
        "    resolution=50,\n",
        "    doi_cut=Cut('distilbert_embeddings_word_embeddings'), \n",
        "    qoi=ComparativeQoI(1, 0), #qoi='max',  \n",
        "    qoi_cut=OutputCut(accessor=lambda o: o['logits'])\n",
        ")"
      ],
      "metadata": {
        "id": "K_f3EmyWX_Hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Input:  integrated gradient calculator: (trulens.nn.attribution.IntegratedGradients())\n",
        "        original text: (str)\n",
        "        perturbed text: (str)\n",
        "Output: attribution for original text: (tensor of shape (?))\n",
        "        attribution for perturbed text: (tensor of shape (?))\n",
        "Function:\n",
        "        encode raw text of an original-adversarial pair into input tokens for the distilbert model\n",
        "        feed through IG\n",
        "        compute metrics\n",
        "'''\n",
        "def calculate_IGs(IG, string):\n",
        "    encode_string = tokenize(string)\n",
        "    input_ids = torch.tensor(encode_string[\"input_ids\"])\n",
        "    attribution = IG.attributions(input_ids.to(device))\n",
        "    \n",
        "    return attribution"
      ],
      "metadata": {
        "id": "pBlMG8ZjYA73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UCuvhhhBYDSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attack code from BERT-Attack"
      ],
      "metadata": {
        "id": "xM5akvVbOu5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "filter_words = ['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'ain', 'all', 'almost',\n",
        "                'alone', 'along', 'already', 'also', 'although', 'am', 'among', 'amongst', 'an', 'and', 'another',\n",
        "                'any', 'anyhow', 'anyone', 'anything', 'anyway', 'anywhere', 'are', 'aren', \"aren't\", 'around', 'as',\n",
        "                'at', 'back', 'been', 'before', 'beforehand', 'behind', 'being', 'below', 'beside', 'besides',\n",
        "                'between', 'beyond', 'both', 'but', 'by', 'can', 'cannot', 'could', 'couldn', \"couldn't\", 'd', 'didn',\n",
        "                \"didn't\", 'doesn', \"doesn't\", 'don', \"don't\", 'down', 'due', 'during', 'either', 'else', 'elsewhere',\n",
        "                'empty', 'enough', 'even', 'ever', 'everyone', 'everything', 'everywhere', 'except', 'first', 'for',\n",
        "                'former', 'formerly', 'from', 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'he', 'hence',\n",
        "                'her', 'here', 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'him', 'himself', 'his',\n",
        "                'how', 'however', 'hundred', 'i', 'if', 'in', 'indeed', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\",\n",
        "                'its', 'itself', 'just', 'latter', 'latterly', 'least', 'll', 'may', 'me', 'meanwhile', 'mightn',\n",
        "                \"mightn't\", 'mine', 'more', 'moreover', 'most', 'mostly', 'must', 'mustn', \"mustn't\", 'my', 'myself',\n",
        "                'namely', 'needn', \"needn't\", 'neither', 'never', 'nevertheless', 'next', 'no', 'nobody', 'none',\n",
        "                'noone', 'nor', 'not', 'nothing', 'now', 'nowhere', 'o', 'of', 'off', 'on', 'once', 'one', 'only',\n",
        "                'onto', 'or', 'other', 'others', 'otherwise', 'our', 'ours', 'ourselves', 'out', 'over', 'per',\n",
        "                'please', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", \"should've\", 'shouldn', \"shouldn't\", 'somehow',\n",
        "                'something', 'sometime', 'somewhere', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs',\n",
        "                'them', 'themselves', 'then', 'thence', 'there', 'thereafter', 'thereby', 'therefore', 'therein',\n",
        "                'thereupon', 'these', 'they', 'this', 'those', 'through', 'throughout', 'thru', 'thus', 'to', 'too',\n",
        "                'toward', 'towards', 'under', 'unless', 'until', 'up', 'upon', 'used', 've', 'was', 'wasn', \"wasn't\",\n",
        "                'we', 'were', 'weren', \"weren't\", 'what', 'whatever', 'when', 'whence', 'whenever', 'where',\n",
        "                'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while',\n",
        "                'whither', 'who', 'whoever', 'whole', 'whom', 'whose', 'why', 'with', 'within', 'without', 'won',\n",
        "                \"won't\", 'would', 'wouldn', \"wouldn't\", 'y', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\",\n",
        "                'your', 'yours', 'yourself', 'yourselves', '-', ',', '.', ':', ';', '(', ')', '\"', \"'\", '|', '...']\n",
        "filter_words = set(filter_words)\n",
        "\n",
        "\n",
        "def get_sim_embed(embed_path, sim_path):\n",
        "    id2word = {}\n",
        "    word2id = {}\n",
        "\n",
        "    with open(embed_path, 'r', encoding='utf-8') as ifile:\n",
        "        for line in ifile:\n",
        "            word = line.split()[0]\n",
        "            if word not in id2word:\n",
        "                id2word[len(id2word)] = word\n",
        "                word2id[word] = len(id2word) - 1\n",
        "\n",
        "    cos_sim = np.load(sim_path)\n",
        "    return cos_sim, word2id, id2word\n",
        "\n",
        "\n",
        "\n",
        "class Feature(object):\n",
        "    def __init__(self, seq_a, label, example_index):\n",
        "        self.example_index = example_index\n",
        "        self.label = label\n",
        "        self.pred = -1\n",
        "        self.seq = seq_a\n",
        "        self.final_adverse = seq_a\n",
        "        self.query = 0\n",
        "        self.change = 0\n",
        "        self.success = 0\n",
        "        self.sim = 0.0\n",
        "        self.metrics = None\n",
        "        self.changes = []\n",
        "        self.attributions = None\n",
        "\n",
        "\n",
        "def _tokenize(seq, tokenizer):\n",
        "    seq = seq.replace('\\n', '').lower()\n",
        "    words = seq.split(' ')\n",
        "\n",
        "    sub_words = []\n",
        "    keys = []\n",
        "    index = 0\n",
        "    for word in words:\n",
        "        sub = tokenizer.tokenize(word)\n",
        "        sub_words += sub\n",
        "        keys.append([index, index + len(sub)])\n",
        "        index += len(sub)\n",
        "\n",
        "    return words, sub_words, keys\n",
        "\n",
        "\n",
        "def _get_masked(words):\n",
        "    len_text = len(words)\n",
        "    masked_words = []\n",
        "    for i in range(len_text - 1):\n",
        "        masked_words.append(words[0:i] + ['[UNK]'] + words[i + 1:])\n",
        "    # list of words\n",
        "    return masked_words\n",
        "\n",
        "\n",
        "def get_important_scores(words, tgt_model, orig_prob, orig_label, orig_probs, tokenizer_tgt, batch_size, max_length):\n",
        "    masked_words = _get_masked(words)\n",
        "    texts = [' '.join(words) for words in masked_words]  # list of text of masked words\n",
        "    all_input_ids = []\n",
        "    all_masks = []\n",
        "    all_segs = []\n",
        "    for text in texts:\n",
        "        inputs = tokenizer_tgt.encode_plus(text, None, add_special_tokens=True, max_length=max_length, return_token_type_ids=True, truncation=True)\n",
        "        input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "        padding_length = max_length - len(input_ids)\n",
        "        input_ids = input_ids + (padding_length * [0])\n",
        "        token_type_ids = token_type_ids + (padding_length * [0])\n",
        "        attention_mask = attention_mask + (padding_length * [0])\n",
        "        all_input_ids.append(input_ids)\n",
        "        all_masks.append(attention_mask)\n",
        "        all_segs.append(token_type_ids)\n",
        "    seqs = torch.tensor(all_input_ids, dtype=torch.long)\n",
        "    masks = torch.tensor(all_masks, dtype=torch.long)\n",
        "    segs = torch.tensor(all_segs, dtype=torch.long)\n",
        "    seqs = seqs.to(device)\n",
        "\n",
        "    eval_data = TensorDataset(seqs)\n",
        "    # Run prediction for full data\n",
        "    eval_sampler = SequentialSampler(eval_data)\n",
        "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=batch_size)\n",
        "    leave_1_probs = []\n",
        "    for batch in eval_dataloader:\n",
        "        masked_input, = batch\n",
        "        bs = masked_input.size(0)\n",
        "\n",
        "        leave_1_prob_batch = tgt_model(masked_input)[0]  # B num-label\n",
        "        leave_1_probs.append(leave_1_prob_batch)\n",
        "    if len(leave_1_probs) == 0:\n",
        "      return []\n",
        "\n",
        "    leave_1_probs = torch.cat(leave_1_probs, dim=0)  # words, num-label\n",
        "    leave_1_probs = torch.softmax(leave_1_probs, -1)  #\n",
        "    leave_1_probs_argmax = torch.argmax(leave_1_probs, dim=-1)\n",
        "    import_scores = (orig_prob\n",
        "                     - leave_1_probs[:, orig_label]\n",
        "                     +\n",
        "                     (leave_1_probs_argmax != orig_label).float()\n",
        "                     * (leave_1_probs.max(dim=-1)[0] - torch.index_select(orig_probs, 0, leave_1_probs_argmax))\n",
        "                     ).data.cpu().numpy()\n",
        "\n",
        "    return import_scores\n",
        "\n",
        "\n",
        "def get_substitues(substitutes, tokenizer, mlm_model, use_bpe, substitutes_score=None, threshold=3.0):\n",
        "    # substitues L,k\n",
        "    # from this matrix to recover a word\n",
        "    words = []\n",
        "    sub_len, k = substitutes.size()  # sub-len, k\n",
        "\n",
        "    if sub_len == 0:\n",
        "        return words\n",
        "\n",
        "    elif sub_len == 1:\n",
        "        for (i,j) in zip(substitutes[0], substitutes_score[0]):\n",
        "            if threshold != 0 and j < threshold:\n",
        "                break\n",
        "            words.append(tokenizer._convert_id_to_token(int(i)))\n",
        "\n",
        "    return words\n",
        "\n",
        "\n",
        "def get_bpe_substitues(substitutes, tokenizer, mlm_model):\n",
        "    # substitutes L, k\n",
        "\n",
        "    substitutes = substitutes[0:5, 0:4] # maximum BPE candidates # 0:12\n",
        "\n",
        "    # find all possible candidates\n",
        "\n",
        "    all_substitutes = []\n",
        "    for i in range(substitutes.size(0)):\n",
        "        if len(all_substitutes) == 0:\n",
        "            lev_i = substitutes[i]\n",
        "            all_substitutes = [[int(c)] for c in lev_i]\n",
        "        else:\n",
        "            lev_i = []\n",
        "            for all_sub in all_substitutes:\n",
        "                for j in substitutes[i]:\n",
        "                    lev_i.append(all_sub + [int(j)])\n",
        "            all_substitutes = lev_i\n",
        "\n",
        "    # all substitutes  list of list of token-id (all candidates)\n",
        "    c_loss = nn.CrossEntropyLoss(reduction='none')\n",
        "    word_list = []\n",
        "    # all_substitutes = all_substitutes[:24]\n",
        "    all_substitutes = torch.tensor(all_substitutes) # [ N, L ]\n",
        "    all_substitutes = all_substitutes[:24].to(device)\n",
        "    N, L = all_substitutes.size()\n",
        "    word_predictions = mlm_model(all_substitutes)[0] # N L vocab-size\n",
        "    ppl = c_loss(word_predictions.view(N*L, -1), all_substitutes.view(-1)) # [ N*L ]\n",
        "    ppl = torch.exp(torch.mean(ppl.view(N, L), dim=-1)) # N\n",
        "    _, word_list = torch.sort(ppl)\n",
        "    word_list = [all_substitutes[i] for i in word_list]\n",
        "    final_words = []\n",
        "    for word in word_list:\n",
        "        tokens = [tokenizer._convert_id_to_token(int(i)) for i in word]\n",
        "        text = tokenizer.convert_tokens_to_string(tokens)\n",
        "        final_words.append(text)\n",
        "    return final_words\n",
        "\n",
        "def num_tokens(seq):\n",
        "  inputs = tokenizer_tgt.encode_plus(seq, None, add_special_tokens=True, max_length=128, return_token_type_ids=True, truncation=True)\n",
        "  return torch.tensor(inputs[\"input_ids\"]).size(0)\n",
        "\n",
        "def top_k_agree(original_attribuion, perturbed_attribution, k=5):\n",
        "  original_magnitudes = np.linalg.norm(original_attribution, ord=1, dim=-1)\n",
        "  perturbed_magnitudes = np.linalg.norm(perturbed_attribution, ord=1, dim=-1)\n",
        "  original_indices = np.argpartition(original_magnitudes, -k)[-k:]  # Indices not sorted\n",
        "  perturbed_indices = np.argpartition(perturbed_magnitudes, -k)[-k:]  # Indices not sorted\n",
        "  return original_indices[np.argsort(original_magnitudes[original_indices])][::-1] == perturbed_indices[np.argsort(perturbed_magnitudes[perturbed_indices])][::-1]\n",
        "\n",
        "def attack(feature, tgt_model, mlm_model, tokenizer_tgt, tokenizer_mlm, k, batch_size, max_length=128, cos_mat=None, w2i={}, i2w={}, use_bpe=1, threshold_pred_score=0.3):\n",
        "    features = []\n",
        "\n",
        "    # Original attribution\n",
        "    original_attribution = calculate_IGs(IG_calc_attribution, feature.seq)\n",
        "    original_attr_summed = torch.sum(original_attribution, dim=-1).flatten()\n",
        "    original_attr_summed_np = list(original_attr_summed.cpu().numpy())\n",
        "\n",
        "    # Get words, subwords, and the indices corresponding to that word in the tokenized sequence\n",
        "    words, sub_words, keys = _tokenize(feature.seq, tokenizer_tgt)\n",
        "\n",
        "    # Get original label (orig_label) and current probability (current_prob)\n",
        "    # Feed in original sequence (feature.seq) into target model\n",
        "    inputs = tokenizer_tgt.encode_plus(feature.seq, None, add_special_tokens=True, max_length=max_length, return_token_type_ids=True, truncation=True)\n",
        "    input_ids, token_type_ids = torch.tensor(inputs[\"input_ids\"]), torch.tensor(inputs[\"token_type_ids\"])\n",
        "    attention_mask = torch.tensor([1] * len(input_ids))\n",
        "    seq_len = input_ids.size(0)\n",
        "    num_tokens_original = seq_len\n",
        "    with torch.no_grad():\n",
        "      orig_probs = tgt_model(input_ids.unsqueeze(0).to(device),\n",
        "                            attention_mask=attention_mask.unsqueeze(0).to(device),\n",
        "                            # token_type_ids=token_type_ids.unsqueeze(0).to(device)\n",
        "                            )[0].squeeze()\n",
        "    orig_probs = torch.softmax(orig_probs, -1)\n",
        "    orig_label = torch.argmax(orig_probs)\n",
        "    current_prob = orig_probs.max()\n",
        "\n",
        "    # Feed subwords into MLM model\n",
        "    sub_words = ['[CLS]'] + sub_words[:max_length - 2] + ['[SEP]']\n",
        "    input_ids_ = torch.tensor([tokenizer_mlm.convert_tokens_to_ids(sub_words)])\n",
        "    # For each subword in the sequence, outputs a prediction (vocab size vector)\n",
        "    with torch.no_grad():\n",
        "        word_predictions = mlm_model(input_ids_.to(device))[0].squeeze()  # seq-len(sub) vocab\n",
        "    # word_pred_scores_all is the top k values, word_predictions is their corresponding indices\n",
        "    word_pred_scores_all, word_predictions = torch.topk(word_predictions, k, -1)  # seq-len k\n",
        "    # Top k predictions (indices) for subwords (other than CLS and SEP)\n",
        "    word_predictions = word_predictions[1:len(sub_words) + 1, :]\n",
        "    # Top k predictions (probabilities) for subwords (other than CLS and SEP)\n",
        "    word_pred_scores_all = word_pred_scores_all[1:len(sub_words) + 1, :]\n",
        "    # Get importance scores for each word\n",
        "    important_scores = get_important_scores(words, tgt_model, current_prob, orig_label, orig_probs,\n",
        "                                            tokenizer_tgt, batch_size, max_length)\n",
        "    # Store number of words??\n",
        "    feature.query += int(len(words))\n",
        "    # Get indices of words in decreasing importance score order\n",
        "    list_of_index = sorted(enumerate(important_scores), key=lambda x: x[1], reverse=True)\n",
        "    final_words = copy.deepcopy(words)\n",
        "\n",
        "    # Iterate through words in decreasing importance score order\n",
        "    for top_index in list_of_index:\n",
        "        # If number of changes exceeds 40% of the whole sequence, return\n",
        "        if feature.change > int(0.4 * (len(words))): # 0.4 is the perturbation percentage\n",
        "            return None\n",
        "\n",
        "        # tgt_word is the word to replace \n",
        "        tgt_word = words[top_index[0]]\n",
        "        if tgt_word in filter_words:\n",
        "            continue\n",
        "        if keys[top_index[0]][0] > max_length - 2:\n",
        "            continue\n",
        "\n",
        "        # For each of L tokens in the word, top k predictions\n",
        "        substitutes = word_predictions[keys[top_index[0]][0]:keys[top_index[0]][1]]  # L, k\n",
        "        word_pred_scores = word_pred_scores_all[keys[top_index[0]][0]:keys[top_index[0]][1]]\n",
        "        # Getting most likely substitutes based on bpe\n",
        "        substitutes = get_substitues(substitutes, tokenizer_mlm, mlm_model, use_bpe, word_pred_scores, threshold_pred_score)\n",
        "\n",
        "        least_similarity  = 1.0\n",
        "        candidate = None\n",
        "        for substitute_ in substitutes:\n",
        "            substitute = substitute_\n",
        "\n",
        "            if substitute == tgt_word:\n",
        "                continue  # filter out original word\n",
        "            if '##' in substitute:\n",
        "                continue  # filter out sub-word\n",
        "\n",
        "            if substitute in filter_words:\n",
        "                continue\n",
        "            if substitute in w2i and tgt_word in w2i:\n",
        "                if cos_mat[w2i[substitute]][w2i[tgt_word]] < 0.4:\n",
        "                    continue\n",
        "\n",
        "\n",
        "            temp_replace = final_words\n",
        "            # Replace word with with substitute \n",
        "            temp_replace[top_index[0]] = substitute\n",
        "            # Convert back to text\n",
        "            temp_text = tokenizer_tgt.convert_tokens_to_string(temp_replace)\n",
        "            # Tokenize with target model tokenizer\n",
        "            inputs = tokenizer_tgt.encode_plus(temp_text, None, add_special_tokens=True, max_length=max_length, )\n",
        "            # Feed into target model\n",
        "            input_ids = torch.tensor(inputs[\"input_ids\"]).unsqueeze(0).to(device)\n",
        "            seq_len = input_ids.size(1)\n",
        "            with torch.no_grad():\n",
        "                temp_prob = tgt_model(input_ids)[0].squeeze()\n",
        "            # Increment query \n",
        "            feature.query += 1\n",
        "            # Get output label\n",
        "            temp_prob = torch.softmax(temp_prob, -1)\n",
        "            temp_label = torch.argmax(temp_prob)\n",
        "            feature.pred = temp_label.detach().cpu().numpy()\n",
        "\n",
        "            # Skip if number of tokens don't match \n",
        "            if num_tokens(temp_text) != num_tokens_original:\n",
        "              continue\n",
        "\n",
        "            # Calculate integrated gradients and similarity between them\n",
        "            perturbed_attribution = calculate_IGs(IG_calc_attribution, temp_text)\n",
        "            perturbed_attr_summed= torch.sum(perturbed_attribution, dim=-1).flatten()\n",
        "            feature.metrics = similarity_metrics(original_attr_summed, perturbed_attr_summed)\n",
        "            feature.attributions = original_attr_summed_np, list(perturbed_attr_summed.cpu().numpy())\n",
        "\n",
        "            if feature.metrics['cosine_sim'] < 0.863:\n",
        "                # If succeeded, increment change\n",
        "                feature.change += 1\n",
        "                # Make the change\n",
        "                final_words[top_index[0]] = substitute\n",
        "                # Append the actual changes to changes\n",
        "                feature.changes.append([keys[top_index[0]][0], substitute, tgt_word])\n",
        "                # Store the adversarial example text in final_adverse \n",
        "                feature.final_adverse = temp_text\n",
        "                feature.success = 4\n",
        "                return feature\n",
        "            else:\n",
        "                # Store the example with the least similarity\n",
        "                if feature.metrics['cosine_sim'] < least_similarity:\n",
        "                    least_similarity = feature.metrics['cosine_sim']\n",
        "                    candidate = substitute\n",
        "                feature.final_adverse = temp_text\n",
        "\n",
        "        if least_similarity < 1.0:\n",
        "            feature.change += 1\n",
        "            feature.changes.append([keys[top_index[0]][0], candidate, tgt_word])\n",
        "            final_words[top_index[0]] = candidate\n",
        "\n",
        "    return None"
      ],
      "metadata": {
        "id": "biupWOto8iUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "DUvDP4cW96U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Load data"
      ],
      "metadata": {
        "id": "JE8CS-xSO5sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_df(df):\n",
        "  df['text'] = df['text'].apply(lambda x: re.sub(r'http[^\\s]*', '', x))\n",
        "  df['text'] = df['text'].apply(lambda x: re.sub('@[^\\s]+','', x))\n",
        "  df['text'] = df['text'].apply(lambda x: re.sub(r'#', '', x))\n",
        "  df['text'] = df['text'].apply(lambda x: re.sub(r'-', '', x))\n",
        "  df['text'] = df['text'].apply(lambda x: x.strip())\n",
        "  return df"
      ],
      "metadata": {
        "id": "UWoxockd7_dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data_cls(data_path):\n",
        "    df = pd.read_csv(data_path)\n",
        "    df = preprocess_df(df)\n",
        "    features = []\n",
        "    seqs = list(df['text'])\n",
        "    labels = list(df['sentiment'])\n",
        "    features = list(zip(seqs, labels))\n",
        "    return features \n",
        "    \n",
        "features = get_data_cls(data_path)\n",
        "print(features[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKww6eBd99M1",
        "outputId": "0777539f-558b-4b36-a017-ec0138ca99f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(\"Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\", 0), (\"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\", 0), ('I dived many times for the ball. Managed to save 50%  The rest go out of bounds', 0), ('my whole body feels itchy and like its on fire', 0), (\"no, it's not behaving at all. i'm mad. why am i here? because I can't see you all over there.\", 0)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Generate adversarial examples"
      ],
      "metadata": {
        "id": "3erDYz15O84h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if use_sim_mat == 1:\n",
        "    cos_mat, w2i, i2w = get_sim_embed('data_defense/counter-fitted-vectors.txt', 'data_defense/cos_sim_counter_fitting.npy')\n",
        "else:\n",
        "    cos_mat, w2i, i2w = None, {}, {}\n",
        "\n",
        "features_output = []\n",
        "\n",
        "# old_time = time.time()\n",
        "\n",
        "start = 0 \n",
        "end = 5000\n",
        "outputs = []\n",
        "\n",
        "for index, feature in enumerate(tqdm(features[start:end])):\n",
        "    seq_a, label = feature\n",
        "    feat = Feature(seq_a, label, index + start)\n",
        "\n",
        "    # print('\\r number {:d} '.format(index) + tgt_path, end='')\n",
        "    feature = attack(feat, tgt_model, mlm_model, tokenizer_tgt, tokenizer_mlm, k, batch_size=32, max_length=128,\n",
        "                  cos_mat=cos_mat, w2i=w2i, i2w=i2w, use_bpe=use_bpe,threshold_pred_score=threshold_pred_score)\n",
        "\n",
        "    if feature:\n",
        "        outputs.append({'example_index': feature.example_index,\n",
        "                        'label': feature.label,\n",
        "                        'prediction': feature.pred,\n",
        "                        'success': feature.success,\n",
        "                        'change': feature.change,\n",
        "                        'num_word': len(feature.seq.split(' ')),\n",
        "                        'query': feature.query,\n",
        "                        'changes': feature.changes,\n",
        "                        'seq_a': feature.seq,\n",
        "                        'adv': feature.final_adverse,\n",
        "                        'original_attribution': feature.attributions[0],\n",
        "                        'perturbed_attribution': feature.attributions[1],\n",
        "                        'cosine_similarity': feature.metrics['cosine_sim'],\n",
        "                        'L1': feature.metrics['L_1'],\n",
        "                        'L2': feature.metrics['L_2'],\n",
        "                        'Linf': feature.metrics['L_inf']\n",
        "                        })\n",
        "        \n",
        "    if index % 100 == 0:\n",
        "      df = pd.DataFrame(outputs)\n",
        "      df.to_csv('adversarial_examples_cosine.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oq4-IR9OoZYf",
        "outputId": "114a5d66-5a73-4335-b30a-a88a2dd24aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/5000 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "100%|| 5000/5000 [1:06:26<00:00,  1.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Save generated examples to csv"
      ],
      "metadata": {
        "id": "KATgCU3lPHjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"adversarial_examples_cosine.csv\"\n",
        "\n",
        "def dump_features(features):\n",
        "    outputs = []\n",
        "\n",
        "    for feature in features:\n",
        "        if feature:\n",
        "            outputs.append({'label': feature.label,\n",
        "                            'prediction': feature.pred,\n",
        "                            'success': feature.success,\n",
        "                            'change': feature.change,\n",
        "                            'num_word': len(feature.seq.split(' ')),\n",
        "                            'query': feature.query,\n",
        "                            'changes': feature.changes,\n",
        "                            'seq_a': feature.seq,\n",
        "                            'adv': feature.final_adverse,\n",
        "                            'original_attribution': feature.attributions[0],\n",
        "                            'perturbed_attribution': feature.attributions[1],\n",
        "                            'cosine_similarity': feature.metrics['cosine_sim'],\n",
        "                            'L1': feature.metrics['L_1'],\n",
        "                            'L2': feature.metrics['L_2'],\n",
        "                            'Linf': feature.metrics['L_inf']\n",
        "                            })\n",
        "\n",
        "    df = pd.DataFrame(outputs)\n",
        "    df.to_csv(output_dir)\n",
        "    print('finished dump')\n",
        "    return df\n",
        "\n",
        "df = dump_features(features_output[:(9000 - 5041)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usqk7nzR8iav",
        "outputId": "27c3e493-0e93-4910-df8d-4b0a5e01ac13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "finished dump\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('adversarial_examples_cosine.csv')\n",
        "len(df) / max(list(df['example_index']))"
      ],
      "metadata": {
        "id": "ZQ8-Em0H0xHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis"
      ],
      "metadata": {
        "id": "NFbPGRvYCLwu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyinflect \n",
        "!pip install -U spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Mu-DcgGtEHo9",
        "outputId": "cff50f9f-3a7e-4fff-d1d2-ae287da4c31b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyinflect\n",
            "  Downloading pyinflect-0.5.1-py3-none-any.whl (703 kB)\n",
            "\u001b[?25l\r\u001b[K     |                               | 10 kB 19.3 MB/s eta 0:00:01\r\u001b[K     |                               | 20 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |                              | 30 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |                              | 40 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |                             | 51 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |                             | 61 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |                            | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |                            | 81 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |                           | 92 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |                           | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                          | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                          | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                          | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                         | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                         | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                        | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                        | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                       | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                       | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                      | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                      | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                     | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                     | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                    | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                    | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                    | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                   | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                   | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                  | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                  | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                 | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                 | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |                | 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |               | 358 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |               | 368 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |              | 378 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |              | 389 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |             | 399 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |             | 409 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |             | 419 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |            | 430 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |            | 440 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |           | 450 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |           | 460 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |          | 471 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |          | 481 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |         | 491 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |         | 501 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |        | 512 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |        | 522 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |       | 532 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |       | 542 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |      | 552 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |      | 563 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |      | 573 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |     | 583 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |     | 593 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |    | 604 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |    | 614 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |   | 624 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |   | 634 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |  | 645 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |  | 655 kB 5.2 MB/s eta 0:00:01\r\u001b[K     | | 665 kB 5.2 MB/s eta 0:00:01\r\u001b[K     | | 675 kB 5.2 MB/s eta 0:00:01\r\u001b[K     || 686 kB 5.2 MB/s eta 0:00:01\r\u001b[K     || 696 kB 5.2 MB/s eta 0:00:01\r\u001b[K     || 703 kB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyinflect\n",
            "Successfully installed pyinflect-0.5.1\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[K     || 6.2 MB 5.1 MB/s \n",
            "\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.9.1)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     || 42 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.16-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (660 kB)\n",
            "\u001b[K     || 660 kB 34.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.27.1)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.64.0)\n",
            "Collecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[K     || 457 kB 53.6 MB/s \n",
            "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     || 181 kB 46.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (21.3)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     || 10.1 MB 38.4 MB/s \n",
            "\u001b[?25hCollecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.21.6)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Collecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     || 58 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Installing collected packages: typing-extensions, catalogue, typer, srsly, smart-open, pydantic, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.2.0\n",
            "    Uninstalling typing-extensions-4.2.0:\n",
            "      Successfully uninstalled typing-extensions-4.2.0\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 1.0.0\n",
            "    Uninstalling catalogue-1.0.0:\n",
            "      Successfully uninstalled catalogue-1.0.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 1.0.5\n",
            "    Uninstalling srsly-1.0.5:\n",
            "      Successfully uninstalled srsly-1.0.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 6.0.0\n",
            "    Uninstalling smart-open-6.0.0:\n",
            "      Successfully uninstalled smart-open-6.0.0\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 2.2.4\n",
            "    Uninstalling spacy-2.2.4:\n",
            "      Successfully uninstalled spacy-2.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0+zzzcolab20220506162203 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "Successfully installed catalogue-2.0.7 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.16 typer-0.4.1 typing-extensions-3.10.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.3.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     || 12.8 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
            "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.16)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (57.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
            "Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.10.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.12)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-3.3.0\n",
            "\u001b[38;5;2m Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyinflect\n",
        "import spacy\n",
        "import ast \n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def tense_change(original_token, perturbed_token):\n",
        "\toriginal_token = nlp(original_token)[0]\n",
        "\tperturbed_token = nlp(perturbed_token)[0]\n",
        "\n",
        "\tif original_token.tag_ in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
        "\t\tif perturbed_token.tag_ in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
        "\t\t\tif original_token._.inflect(\"VB\") == perturbed_token._.inflect(\"VB\"):\n",
        "\t\t\t\treturn True \n",
        "\treturn False\n",
        "\n",
        "def semantic_similarity(original_token, perturbed_token):\n",
        "\toriginal_token = nlp(original_token)[0]\n",
        "\tperturbed_token = nlp(perturbed_token)[0]\n",
        "\n",
        "\treturn original_token.similarity(perturbed_token)\n",
        "\n",
        "def is_synonym(original_token, perturbed_token):\n",
        "  perturbed_token_lemma = nlp(perturbed_token)[0].lemma_\n",
        "  for synset in wordnet.synsets(original_token):\n",
        "    for lemma in synset.lemma_names():\n",
        "      if perturbed_token_lemma == lemma:\n",
        "        return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "z6GHKjMBC68u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Postprocess data with different heuristics \n",
        "all_tense_changes = []\n",
        "all_semantic_similarities = []\n",
        "all_synonyms = []\n",
        "\n",
        "for i, d in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "  # if i % 100 == 0:\n",
        "  #   print(\"i\", i)\n",
        "  tense_changes = []\n",
        "  semantic_similarities = []\n",
        "  synonyms = []\n",
        "  for change in ast.literal_eval(d['changes']):\n",
        "    token_index, perturbed, original = change\n",
        "    tense_changed = tense_change(original, perturbed)\n",
        "    tense_changes.append(1 if tense_changed else 0)\n",
        "    semantic_similarities.append(semantic_similarity(original, perturbed))\n",
        "    synonyms.append(1 if is_synonym(original, perturbed) and (not tense_changed) else 0)\n",
        "  assert len(ast.literal_eval(d['changes'])) == len(tense_changes)\n",
        "  assert len(ast.literal_eval(d['changes'])) == len(semantic_similarities)\n",
        "  assert len(ast.literal_eval(d['changes'])) == len(synonyms)\n",
        "  all_tense_changes.append(tense_changes)\n",
        "  all_semantic_similarities.append(semantic_similarities)\n",
        "  all_synonyms.append(synonyms)\n",
        "\n",
        "df['tense_change'] = all_tense_changes\n",
        "df['semantic_similarity'] =all_semantic_similarities\n",
        "df['synonym'] = all_synonyms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOGaTsgPEuA8",
        "outputId": "3ce88cbb-4e2d-4332-bb17-d1c7083ec29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/6206 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "100%|| 6206/6206 [07:58<00:00, 12.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('adversarial_examples_cosine_postprocessed.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNv9SXtKCMMP",
        "outputId": "8eb7a33e-e7f1-4d07-a404-a025102c7d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of generated adversarial examples: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns "
      ],
      "metadata": {
        "id": "uvbSTnScZt9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bounds = np.linspace(0.137, 2, num=31)\n",
        "df['bin'] = np.digitize(np.array(df['cosine_distance']), bounds, right=True)"
      ],
      "metadata": {
        "id": "XEQGvjEe09x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tense_change_none'] = df['tense_change'].apply(lambda x: np.all(1-np.array(x)).astype(int)) \n",
        "df['semantic_similarity_min'] = df['semantic_similarity'].apply(lambda x: min(x))  \n",
        "df['semantic_similarity_max'] = df['semantic_similarity'].apply(lambda x: max(x))  \n",
        "df['semantic_similarity_mean'] = df['semantic_similarity'].apply(lambda x: np.mean(x))  \n",
        "df['semantic_similarity_median'] = df['semantic_similarity'].apply(lambda x: np.median(x))  \n",
        "df['synonym_all'] = df['synonym'].apply(lambda x: np.all(x).astype(int))  \n",
        "df_agg = df.groupby('bin').agg({'tense_change_none': 'mean', 'synonym_all': 'mean', 'semantic_similarity_mean': 'mean', 'cosine_distance': 'mean'})\n",
        "df_agg['bin'] = df_agg.index"
      ],
      "metadata": {
        "id": "WlnQsfUn094x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def colors_from_values(values, palette_name):\n",
        "    # normalize the values to range [0, 1]\n",
        "    normalized = (values - min(values)) / (max(values) - min(values))\n",
        "    # convert to indices\n",
        "    indices = np.round(normalized * (len(values) - 1)).astype(np.int32)\n",
        "    # use the indices to get the colors\n",
        "    palette = sns.color_palette(palette_name, len(values))\n",
        "    return np.array(palette).take(indices, axis=0)"
      ],
      "metadata": {
        "id": "h1DAcVcG0-BO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_context('poster')\n",
        "plt.figure(figsize=(12, 4))\n",
        "ax = sns.barplot(x='bin', y='tense_change_none', data=df_agg, palette=colors_from_values(df_agg['tense_change_none'], \"magma\"))\n",
        "ax.set(xticklabels=[])\n",
        "ax.set_ylim(0.7, 1)\n",
        "ax.set_xlabel('Discretized dissimilarity between \\n attributions (cosine distance)')\n",
        "ax.set_ylabel('% of example \\n pairs with \\n consistent tense')\n",
        "ax.xaxis.label.set_color('#cc0000')\n",
        "ax.yaxis.label.set_color('#3c78d8')\n",
        "ax.tick_params(bottom=False)\n"
      ],
      "metadata": {
        "id": "Zl55mi-c0-D8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "ax = sns.barplot(x='bin', y='synonym_all', data=df_agg, palette=colors_from_values(df_agg['synonym_all'], \"magma\"))\n",
        "ax.set(xticklabels=[])\n",
        "ax.set_ylim(0, 0.2)\n",
        "ax.set_xlabel('Discretized dissimilarity between \\n attributions (cosine distance)')\n",
        "ax.xaxis.label.set_color('#cc0000')\n",
        "ax.yaxis.label.set_color('#3c78d8')\n",
        "ax.set_ylabel('% of example pairs \\n with only synonym \\n substitutions')\n",
        "ax.tick_params(bottom=False)"
      ],
      "metadata": {
        "id": "6bXQb-Zi1GyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Ky9qMtqG0-Gj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Check accuracy of target model"
      ],
      "metadata": {
        "id": "BpRSTmSlTGv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct = 0\n",
        "for seq, label in features[start:end]: \n",
        "  inputs = tokenizer_tgt.encode_plus(seq, None, add_special_tokens=True, max_length=128, return_token_type_ids=True, truncation=True)\n",
        "  input_ids, token_type_ids = torch.tensor(inputs[\"input_ids\"]), torch.tensor(inputs[\"token_type_ids\"])\n",
        "  attention_mask = torch.tensor([1] * len(input_ids))\n",
        "  seq_len = input_ids.size(0)\n",
        "  probs = tgt_model(input_ids.unsqueeze(0).to(device),\n",
        "                          attention_mask=attention_mask.unsqueeze(0).to(device),\n",
        "                          )[0].squeeze()\n",
        "  probs = torch.softmax(probs, -1)\n",
        "  pred_label = torch.argmax(probs)\n",
        "  if label == pred_label:\n",
        "    num_correct += 1 \n",
        "\n",
        "print(\"Accuracy: {}\".format(num_correct / len(features[start:end])))\n"
      ],
      "metadata": {
        "id": "XapDgs8cOGwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Old code for running `bertattack.py`"
      ],
      "metadata": {
        "id": "cxh7U-kaPKaO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess    "
      ],
      "metadata": {
        "id": "F8rRjJ0e8bwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = f\"python3 bertattack.py --data_path tweet_sentiment_pairs.csv\\\n",
        "                                                 --mlm_path bert-base-uncased\\\n",
        "                                                 --tgt_path siebert/sentiment-roberta-large-english\\\n",
        "                                                  --use_sim_mat 0\\\n",
        "                                                  --output_dir data_defense/imdb_logs.tsv\\\n",
        "                                                  --num_label 2\\\n",
        "                                                  --use_bpe 1\\\n",
        "                                                  --k 48\\\n",
        "                                                  --start 0\\\n",
        "                                                  --end 1000\\\n",
        "                                                  --threshold_pred_score 0\""
      ],
      "metadata": {
        "id": "qvfQLJsR0P44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!{run}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wja6a1D40f5G",
        "outputId": "512aeb7a-b3af-40fc-e7ec-bc9325a8f867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start process\n",
            "Downloading: 100% 226k/226k [00:00<00:00, 272kB/s]\n",
            "Downloading: 100% 28.0/28.0 [00:00<00:00, 18.2kB/s]\n",
            "Downloading: 100% 570/570 [00:00<00:00, 532kB/s]\n",
            "Downloading: 100% 256/256 [00:00<00:00, 242kB/s]\n",
            "Downloading: 100% 687/687 [00:00<00:00, 574kB/s]\n",
            "Downloading: 100% 780k/780k [00:01<00:00, 738kB/s] \n",
            "Downloading: 100% 446k/446k [00:01<00:00, 422kB/s]\n",
            "Downloading: 100% 150/150 [00:00<00:00, 116kB/s]\n",
            "Downloading: 100% 420M/420M [00:06<00:00, 71.9MB/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Downloading: 100% 1.32G/1.32G [01:14<00:00, 19.1MB/s]\n",
            "loading sim-embed\n",
            "finish get-sim-embed\n",
            " number 0 siebert/sentiment-roberta-large-englishTraceback (most recent call last):\n",
            "  File \"bertattack.py\", line 512, in <module>\n",
            "    run_attack()\n",
            "  File \"bertattack.py\", line 497, in run_attack\n",
            "    cos_mat=cos_mat, w2i=w2i, i2w=i2w, use_bpe=use_bpe,threshold_pred_score=threshold_pred_score)\n",
            "  File \"bertattack.py\", line 259, in attack\n",
            "    tokenizer_tgt, batch_size, max_length)\n",
            "  File \"bertattack.py\", line 150, in get_important_scores\n",
            "    leave_1_probs = torch.cat(leave_1_probs, dim=0)  # words, num-label\n",
            "NotImplementedError: There were no tensor arguments to this function (e.g., you passed an empty list of Tensors), but no fallback function is registered for schema aten::_cat.  This usually means that this function requires a non-empty list of Tensors, or that you (the operator writer) forgot to register a fallback function.  Available functions are [CPU, CUDA, QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, AutocastCPU, Autocast, Batched, VmapMode, Functionalize].\n",
            "\n",
            "CPU: registered at aten/src/ATen/RegisterCPU.cpp:21063 [kernel]\n",
            "CUDA: registered at aten/src/ATen/RegisterCUDA.cpp:29726 [kernel]\n",
            "QuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:1258 [kernel]\n",
            "BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\n",
            "Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:47 [backend fallback]\n",
            "Named: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\n",
            "Conjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:18 [backend fallback]\n",
            "Negative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\n",
            "ZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\n",
            "ADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:64 [backend fallback]\n",
            "AutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\n",
            "AutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\n",
            "AutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\n",
            "AutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\n",
            "AutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\n",
            "AutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\n",
            "AutogradMLC: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\n",
            "AutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\n",
            "AutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\n",
            "AutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\n",
            "AutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\n",
            "AutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_3.cpp:11380 [autograd kernel]\n",
            "Tracer: registered at ../torch/csrc/autograd/generated/TraceType_3.cpp:11220 [kernel]\n",
            "AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:461 [backend fallback]\n",
            "Autocast: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:305 [backend fallback]\n",
            "Batched: registered at ../aten/src/ATen/BatchingRegistrations.cpp:1059 [backend fallback]\n",
            "VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
            "Functionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:52 [backend fallback]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ihi7a_440-gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "plfxogLY1t3g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}